             Learning Efﬁcient Convolutional Networks through Network Slimming


         Zhuang Liu 1∗ Jianguo Li 2  Zhiqiang Shen 3  Gao Huang 4  Shoumeng Yan 2  Changshui Zhang 1
          1 CSAI, TNList, Tsinghua University 2 Intel Labs China 3 Fudan University 4 Cornell University
              {liuzhuangthu, zhiqiangshen0214}@gmail.com,{jianguo.li, shoumeng.yan}@intel.com,
                               gh349@cornell.edu, zcs@mail.tsinghua.edu.cn



                        Abstract                     However, larger CNNs, although with stronger represen-
                                                   tation power, are more resource-hungry. For instance, a
          The deployment of deep convolutional neural networks   152-layer ResNet [14] has more than 60 million parame-
        (CNNs) in many real world applications is largely hindered   ters and requires more than 20 Giga ﬂoat-point-operations
        by their high computational cost. In this paper, we propose   (FLOPs) when inferencing an image with resolution 224×
        a novel learning scheme for CNNs to simultaneously 1) re-   224. This is unlikely to be affordable on resource con-
        duce the model size; 2) decrease the run-time memory foot-   strained platforms such as mobile devices, wearables or In-
        print; and 3) lower the number of computing operations,   ternet of Things (IoT) devices.
        without compromising accuracy. This is achieved by en-     The deployment of CNNs in real world applications areforcing channel-level sparsity in the network in a simple but   mostly constrained by1) Model size: CNNs’ strong repre-effective way. Different from many existing approaches, the   sentation power comes from their millions of trainable pa-proposed method directly applies to modern CNN architec-   rameters. Those parameters, along with network structuretures, introduces minimum overhead to the training process,   information, need to be stored on disk and loaded into mem-and requires no special software/hardware accelerators for   ory during inference time. As an example, storing a typi-the resulting models. We call our approachnetwork slim-   cal CNN trained on ImageNet consumes more than 300MBming, which takes wide and large networks as input mod-   space, which is a big resource burden to embedded devices.els, but during training insigniﬁcant channels are automat-   2) Run-time memory: During inference time, the interme-ically identiﬁed and pruned afterwards, yielding thin and   diate activations/responses of CNNs could even take morecompact models with comparable accuracy. We empirically   memory space than storing the model parameters, even withdemonstrate the effectiveness of our approach with several   batch size 1. This is not a problem for high-end GPUs, butstate-of-the-art CNN models, including VGGNet, ResNet   unaffordable for many applications with low computationaland DenseNet, on various image classiﬁcation datasets. For   power.3) Number of computing operations:The convolu-VGGNet, a multi-pass version of network slimming gives a   tion operations are computationally intensive on high reso-20×reduction in model size and a 5×reduction in comput-   lution images. A large CNN may take several minutes toing operations.                                 process one single image on a mobile device, making it un-
                                                   realistic to be adopted for real applications.
        1. Introduction                                Many works have been proposed to compress large
                                                   CNNs or directly learn more efﬁcient CNN models for fast
          In recent years, convolutional neural networks (CNNs)   inference. These include low-rank approximation [7], net-
        have become the dominant approach for a variety of com-   work quantization [3, 12] and binarization [28, 6], weight
        puter vision tasks, e.g., image classiﬁcation [22], object   pruning [12], dynamic inference [16], etc. However, most
        detection [8], semantic segmentation [26]. Large-scale   of these methods can only address one or two challenges
        datasets, high-end modern GPUs and new network architec-   mentioned above. Moreover, some of the techniques require
        tures allow the development of unprecedented large CNN   specially designed software/hardware accelerators for exe-
        models. For instance, from AlexNet [22], VGGNet [31] and   cution speedup [28, 6, 12].
        GoogleNet [34] to ResNets [14], the ImageNet Classiﬁca-     Another direction to reduce the resource consumption of
        tion Challenge winner models have evolved from 8 layers   large CNNs is to sparsify the network. Sparsity can be im-
        to more than 100 layers.                           posed on different level of structures [2, 37, 35, 29, 25],
          ∗ This work was done when Zhuang Liu and Zhiqiang Shen were interns   which yields considerable model-size compression and in-
        at Intel Labs China. Jianguo Li is the corresponding author.           ference speedup. However, these approaches generally re-



                                                 2736                      channel scaling                                channel scaling  i-thconv-layer   factors        (i+1)=j-th         i-thconv-layer    factors       (i+1)=j-th
                                     conv-layer                                  conv-layer Ci1          1.170                           C           1.170
               C                       C                 i1
                i2          0.001           j1                                        Cj1
               Ci3          0.290                 pruning     Ci3          0.290
               C          0.003          Ci4                        j2                                        Cj2
                                                          …      …    …
                       …    …
                …

               C                                        Cin          0.820 in          0.820
                    initial network                             compact network
        Figure 1: We associate a scaling factor (reused from a batch normalization layer) with each channel in convolutional layers. Sparsity
        regularization is imposed on these scaling factors during training to automatically identify unimportant channels. The channels with small
        scaling factor values (in orange color) will be pruned (left side). After pruning, we obtain compact models (right side), which are then
        ﬁne-tuned to achieve comparable (or even higher) accuracy as normally trained full network.

        quire special software/hardware accelerators to harvest the   Low-rank Decompositionapproximates weight matrix in
        gain in memory or time savings, though it is easier than   neural networks with low-rank matrix using techniques like
        non-structured sparse weight matrix as in [12].            Singular Value Decomposition (SVD) [7]. This method
          In this paper, we proposenetwork slimming, a simple   works especially well on fully-connected layers, yield-
        yet effective network training scheme, which addresses all   ing∼3x model-size compression however without notable
        the aforementioned challenges when deploying large CNNs   speed acceleration, since computing operations in CNN
        under limited resources. Our approach imposes L1 regular-   mainly come from convolutional layers.
        ization on the scaling factors in batch normalization (BN)   Weight Quantization. HashNet [3] proposes to quantizelayers, thus it is easy to implement without introducing any   the network weights. Before training, network weights arechange to existing CNN architectures. Pushing the val-   hashed to different groups and within each group weightues of BN scaling factors towards zero with L1 regulariza-   the value is shared. In this way only the shared weights andtion enables us to identify insigniﬁcant channels (or neu-   hash indices need to be stored, thus a large amount of stor-rons), as each scaling factor corresponds to a speciﬁc con-   age space could be saved. [12] uses a improved quantizationvolutional channel (or a neuron in a fully-connected layer).   technique in a deep compression pipeline and achieves 35xThis facilitates the channel-level pruning at the followed   to 49x compression rates on AlexNet and VGGNet. How-step. The additional regularization term rarely hurt the per-   ever, these techniques can neither save run-time memoryformance. In fact, in some cases it leads to higher gen-   nor inference time, since during inference shared weightseralization accuracy. Pruning unimportant channels may   need to be restored to their original positions.sometimes temporarily degrade the performance, but this     [28, 6] quantize real-valued weights into binary/ternaryeffect can be compensated by the followed ﬁne-tuning of   weights (weight values restricted to{−1,1}or{−1,0,1}).the pruned network. After pruning, the resulting narrower   This yields a large amount of model-size saving, and signiﬁ-network is much more compact in terms of model size, run-   cant speedup could also be obtained given bitwise operationtime memory, and computing operations compared to the   libraries. However, this aggressive low-bit approximationinitial wide network. The above process can be repeated   method usually comes with a moderate accuracy loss. for several times, yielding a multi-pass network slimming
        scheme which leads to even more compact network.        Weight Pruning / Sparsifying.[12] proposes to prune the
          Experiments on several benchmark datasets and different   unimportant connections with small weights in trained neu-
        network architectures show that we can obtain CNN models   ral networks. The resulting network’s weights are mostly
        with up to 20x mode-size compression and 5x reduction in   zeros thus the storage space can be reduced by storing the
        computing operations of the original ones, while achieving   model in a sparse format. However, these methods can only
        the same or even higher accuracy. Moreover, our method   achieve speedup with dedicated sparse matrix operation li-
        achieves model compression and inference speedup with   braries and/or hardware. The run-time memory saving is
        conventional hardware and deep learning software pack-   also very limited since most memory space is consumed by
        ages, since the resulting narrower model is free of any   the activation maps (still dense) instead of the weights.
        sparse storing format or computing operations.              In [12], there is no guidance for sparsity during training.
                                                   [32] overcomes this limitation by explicitly imposing sparse
        2. Related Work                             constraint over each weight with additional gate variables,
                                                   and achieve high compression rates by pruning connections
          In this section, we discuss related work from ﬁve aspects.   with zero gate values. This method achieves better com-



                                                 2737        pression rate than [12], but suffers from the same drawback.   Advantages of Channel-level Sparsity. As discussed in
                                                   prior works [35, 23, 11], sparsity can be realized at differ-Structured Pruning / Sparsifying. Recently, [23] pro-   ent levels, e.g., weight-level, kernel-level, channel-level orposes to prune channels with small incoming weights in   layer-level. Fine-grained level (e.g., weight-level) sparsitytrained CNNs, and then ﬁne-tune the network to regain   gives the highest ﬂexibility and generality leads to higheraccuracy. [2] introduces sparsity by random deactivat-   compression rate, but it usually requires special software oring input-output channel-wise connections in convolutional   hardware accelerators to do fast inference on the sparsiﬁedlayers before training, which also yields smaller networks   model [11]. On the contrary, the coarsest layer-level spar-with moderate accuracy loss. Compared with these works,   sity does not require special packages to harvest the infer-we explicitly impose channel-wise sparsity in the optimiza-   ence speedup, while it is less ﬂexible as some whole layerstion objective during training, leading to smoother channel   need to be pruned. In fact, removing layers is only effec-pruning process and little accuracy loss.                tive when the depth is sufﬁciently large, e.g., more than 50[37] imposes neuron-level sparsity during training thus   layers [35, 18]. In comparison, channel-level sparsity pro-some neurons could be pruned to obtain compact networks.   vides a nice tradeoff between ﬂexibility and ease of imple-[35] proposes a Structured Sparsity Learning (SSL) method   mentation. It can be applied to any typical CNNs or fully-to sparsify different level of structures (e.g. ﬁlters, channels   connected networks (treat each neuron as a channel), andor layers) in CNNs. Both methods utilize group sparsity   the resulting network is essentially a “thinned” version ofregualarization during training to obtain structured spar-   the unpruned network, which can be efﬁciently inferenced sity. Instead of resorting to group sparsity on convolu-   on conventional CNN platforms.tional weights, our approach imposes simple L1 sparsity on
        channel-wise scaling factors, thus the optimization objec-   Challenges.  Achieving channel-level sparsity requires
        tive is much simpler.                             pruning all the incoming and outgoing connections asso-
          Since these methods prune or sparsify part of the net-   ciated with a channel. This renders the method of directly
        work structures (e.g., neurons, channels) instead of individ-   pruning weights on a pre-trained model ineffective, as it is
        ual weights, they usually require less specialized libraries   unlikely that all the weights at the input or output end of
        (e.g. for sparse computing operation) to achieve inference   a channel happen to have near zero values. As reported in
        speedup and run-time memory saving. Our network slim-   [23], pruning channels on pre-trained ResNets can only lead
        ming also falls into this category, with absolutely no special   to a reduction of∼10% in the number of parameters without
        libraries needed to obtain the beneﬁts.                 suffering from accuracy loss. [35] addresses this problem
                                                   by enforcing sparsity regularization into the training objec-Neural Architecture Learning. While state-of-the-art   tive. Speciﬁcally, they adoptgroup LASSOto push all theCNNs are typically designed by experts [22, 31, 14], there   ﬁlter weights corresponds to the same channel towards zeroare also some explorations on automatically learning net-   simultaneously during training. However, this approach re-work architectures. [20] introduces sub-modular/super-   quires computing the gradients of the additional regulariza-modular optimization for network architecture search with   tion term with respect to all the ﬁlter weights, which is non-a given resource budget. Some recent works [38, 1] propose   trivial. We introduce a simple idea to address the aboveto learn neural architecture automatically with reinforce-   challenges, and the details are presented below.ment learning. The searching space of these methods are
        extremely large, thus one needs to train hundreds of mod-   Scaling Factors and Sparsity-induced Penalty.Our idea
        els to distinguish good from bad ones. Network slimming   is introducing a scaling factorγfor each channel, which is
        can also be treated as an approach for architecture learning,   multiplied to the output of that channel. Then we jointly
        despite the choices are limited to the width of each layer.   train the network weights and these scaling factors, with
        However, in contrast to the aforementioned methods, net-   sparsity regularization imposed on the latter. Finally we
        work slimming learns network architecture through only a   prune those channels with small factors, and ﬁne-tune the
        single training process, which is in line with our goal of   pruned network. Speciﬁcally, the training objective of our
        efﬁciency.                                    approach is given by
                                                                             
        3. Network slimming                                L=   l(f(x,W),y) +λ   g(γ)     (1)
                                                              (x,y)            γ∈Γ We aim to provide a simple scheme to achieve channel-
        level sparsity in deep CNNs. In this section, we ﬁrst dis-   where(x,y)denote the train input and target,Wdenotes
        cuss the advantages and challenges of channel-level spar-   the trainable weights, the ﬁrst sum-term corresponds to the
        sity, and introduce how we leverage the scaling layers in   normal training loss of a CNN,g(·)is a sparsity-induced
        batch normalization to effectively identify and prune unim-   penalty on the scaling factors, andλbalances the two terms.
        portant channels in the network.                     In our experiment, we chooseg(s) =|s|, which is known as



                                                 2738                                                   convolution layers. 2), if we insert a scaling layer before
                                                   a BN layer, the scaling effect of the scaling layer will be
                 Train with     Prune channels Initial                        Fine-tune the     Compact      completely canceled by the normalization process in BN. channel sparsity     with small network                     pruned network   networkregularization   scaling factors                    3), if we insert scaling layer after BN layer, there are two
                                                   consecutive scaling factors for each channel. Figure 2: Flow-chart of network slimming procedure. The dotted-
        line is for the multi-pass/iterative scheme.                  Channel Pruning and Fine-tuning.After training under
                                                   channel-level sparsity-induced regularization, we obtain a
        L1-norm and widely used to achieve sparsity. Subgradient   model in which many scaling factors are near zero (see Fig-
        descent is adopted as the optimization method for the non-   ure 1). Then we can prune channels with near-zero scaling
        smooth L1 penalty term. An alternative option is to replace   factors, by removing all their incoming and outgoing con-
        the L1 penalty with the smooth-L1 penalty [30] to avoid   nections and corresponding weights. We prune channels
        using sub-gradient at non-smooth point.                with a global threshold across all layers, which is deﬁned
          As pruning a channel essentially corresponds to remov-   as a certain percentile of all the scaling factor values. For
        ing all the incoming and outgoing connections of that chan-   instance, we prune 70% channels with lower scaling factors
        nel, we can directly obtain a narrow network (see Figure 1)   by choosing the percentile threshold as 70%. By doing so,
        without resorting to any special sparse computation pack-   we obtain a more compact network with less parameters and
        ages. The scaling factors act as the agents for channel se-   run-time memory, as well as less computing operations.
        lection. As they are jointly optimized with the network     Pruning may temporarily lead to some accuracy loss,
        weights, the network can automatically identity insigniﬁ-   when the pruning ratio is high. But this can be largely com-
        cant channels, which can be safely removed without greatly   pensated by the followed ﬁne-tuning process on the pruned
        affecting the generalization performance.               network. In our experiments, the ﬁne-tuned narrow network
        Leveraging the Scaling Factors in BN Layers.Batch nor-   can even achieve higher accuracy than the original unpruned
        malization [19] has been adopted by most modern CNNs   network in many cases.
        as a standard approach to achieve fast convergence and bet-   Multi-pass Scheme. We can also extend the proposedter generalization performance. The way BN normalizes   method from single-pass learning scheme (training withthe activations motivates us to design a simple and efﬁ-   sparsity regularization, pruning, and ﬁne-tuning) to a multi-cient method to incorporates the channel-wise scaling fac-   pass scheme. Speciﬁcally, a network slimming proceduretors. Particularly, BN layer normalizes the internal activa-   results in a narrow network, on which we could again applytions using mini-batch statistics. Letzin andzout be the   the whole training procedure to learn an even more compactinput and output of a BN layer,Bdenotes the current mini-   model. This is illustrated by the dotted-line in Figure 2. Ex-batch, BN layer performs the following transformation:      perimental results show that this multi-pass scheme can lead
                                                   to even better results in terms of compression rate.zzˆ= in −µ    B ; zσ2 +ǫ  out =γzˆ+β       (2)   Handling Cross Layer Connections and Pre-activation B                           Structure.  The network slimming process introduced
        whereµB andσB are the mean and standard deviation val-   above can be directly applied to most plain CNN architec-
        ues of input activations overB,γandβare trainable afﬁne   tures such as AlexNet [22] and VGGNet [31]. While some
        transformation parameters (scale and shift) which provides   adaptations are required when it is applied to modern net-
        the possibility of linearly transforming normalized activa-   works withcross layer connectionsand thepre-activation
        tions back to any scales.                           design such as ResNet [15] and DenseNet [17]. For these
          It is common practice to insert a BN layer after a convo-   networks, the output of a layer may be treated as the input
        lutional layer, with channel-wise scaling/shifting parame-   of multiple subsequent layers, in which a BN layer is placed
        ters. Therefore, we can directly leverage theγparameters in   before the convolutional layer. In this case, the sparsity is
        BN layers as the scaling factors we need for network slim-   achieved at the incoming end of a layer, i.e., the layer selec-
        ming. It has the great advantage of introducing no overhead   tively uses a subset of channels it received. To harvest the
        to the network. In fact, this is perhaps also the most effec-   parameter and computation savings at test time, we need
        tive way we can learn meaningful scaling factors for chan-   to place achannel selectionlayer to mask out insigniﬁcant
        nel pruning.1), if we add scaling layers to a CNN without   channels we have identiﬁed.
        BN layer, the value of the scaling factors are not meaning-
        ful for evaluating the importance of a channel, because both   4. Experiments convolution layers and scaling layers are linear transforma-
        tions. One can obtain the same results by decreasing the     We empirically demonstrate the effectiveness of network
        scaling factor values while amplifying the weights in the   slimming on several benchmark datasets. We implement



                                                 2739                                         (a) Test Errors on CIFAR-10
                          Model        Test error (%) Parameters Pruned FLOPs Pruned
                    VGGNet (Baseline)          6.34 20.04M - 7.97×10 8    -
                    VGGNet (70% Pruned)       6.20      2.30M 88.5% 3.91×10 8  51.0%
                    DenseNet-40 (Baseline)       6.11 1.02M - 5.33×10 8    -
                    DenseNet-40 (40% Pruned)     5.19      0.66M 35.7% 3.81×10 8  28.4%
                    DenseNet-40 (70% Pruned)     5.65 0.35M 65.2% 2.40×10 8  55.0%
                    ResNet-164 (Baseline)        5.42 1.70M - 4.99×10 8    -
                    ResNet-164 (40% Pruned)      5.08      1.44M 14.9% 3.81×10 8  23.7%
                    ResNet-164 (60% Pruned)      5.27 1.10M 35.2% 2.75×10 8  44.9%

                                         (b) Test Errors on CIFAR-100
                          Model        Test error (%) Parameters Pruned FLOPs Pruned
                    VGGNet (Baseline)         26.74 20.08M - 7.97×10 8    -
                    VGGNet (50% Pruned)       26.52      5.00M 75.1% 5.01×10 8  37.1%
                    DenseNet-40 (Baseline)       25.36 1.06M - 5.33×10 8    -
                    DenseNet-40 (40% Pruned)    25.28      0.66M 37.5% 3.71×10 8  30.3%
                    DenseNet-40 (60% Pruned)    25.72 0.46M 54.6% 2.81×10 8  47.1%
                    ResNet-164 (Baseline)       23.37 1.73M - 5.00×10 8    -
                    ResNet-164 (40% Pruned)     22.87      1.46M 15.5% 3.33×10 8  33.3%
                    ResNet-164 (60% Pruned)     23.91 1.21M 29.7% 2.47×10 8  50.6%
                                          (c) Test Errors on SVHN
                          Model        Test Error (%) Parameters Pruned FLOPs Pruned
                    VGGNet (Baseline)          2.17 20.04M - 7.97×10 8    -
                    VGGNet (60% Pruned)       2.06      3.04M 84.8% 3.98×10 8  50.1%
                    DenseNet-40 (Baseline)       1.89 1.02M - 5.33×10 8    -
                    DenseNet-40 (40% Pruned)     1.79      0.65M 36.3% 3.69×10 8  30.8%
                    DenseNet-40 (60% Pruned)     1.81 0.44M 56.6% 2.67×10 8  49.8%
                    ResNet-164 (Baseline)        1.78      1.70M - 4.99×10 8    -
                    ResNet-164 (40% Pruned)      1.85 1.46M 14.5% 3.44×10 8  31.1%
                    ResNet-164 (60% Pruned)      1.81 1.12M 34.3% 2.25×10 8  54.9%
        Table 1: Results on CIFAR and SVHN datasets. “Baseline” denotes normal training without sparsity regularization. In column-1, “60%
        pruned” denotes the ﬁne-tuned model with 60% channels pruned from the model trained with sparsity, etc. The pruned ratio of parameters
        and FLOPs are also shown in column-4&6. Pruning a moderate amount (40%) of channels can mostly lower the test errors. The accuracy
        could typically be maintained with≥60% channels pruned.

        our method based on the publicly available Torch [5] im-   images, from which we split a validation set of 6,000 im-
        plementation for ResNets by [10]. The code is available at   ages for model selection during training. The test set con-
        https://github.com/liuzhuang13/slimming.   tains 26,032 images. During training, we select the model
                                                   with the lowest validation error as the model to be pruned
        4.1. Datasets                                 (or the baseline model). We also report the test errors of the
                                                   models with lowest validation errors during ﬁne-tuning.CIFAR.The two CIFAR datasets [21] consist of natural im-
        ages with resolution 32×32. CIFAR-10 is drawn from 10
        and CIFAR-100 from 100 classes. The train and test sets   ImageNet. The ImageNet dataset contains 1.2 millioncontain 50,000 and 10,000 images respectively. On CIFAR-   training images and 50,000 validation images of 100010, a validation set of 5,000 images is split from the training   classes. We adopt the data augmentation scheme as in [10].set for the search ofλ(in Equation 1) on each model. We   We report the single-center-crop validation error of the ﬁnalreport the ﬁnal test errors after training or ﬁne-tuning on   model.all training images. A standard data augmentation scheme
        (shifting/mirroring) [14, 18, 24] is adopted. The input data
        is normalized using channel means and standard deviations.   MNIST.MNIST is a handwritten digit dataset containingWe also compare our method with [23] on CIFAR datasets.   60,000 training images and 10,000 test images. To test the
        SVHN.The Street View House Number (SVHN) dataset   effectiveness of our method on a fully-connected network
        [27] consists of 32x32 colored digit images. Following   (treating each neuron as a channel with 1×1 spatial size),
        common practice [9, 18, 24] we use all the 604,388 training   we compare our method with [35] on this dataset.



                                                 2740        4.2. Network Models                                     Model Parameter and FLOP Savings
          On CIFAR and SVHN dataset, we evaluate our method        100  100.0% 100.0% 100.0%  Original
                                                                                 Parameter Ratio
        on three popular network architectures: VGGNet[31],        80                        FLOPs Ratio
        ResNet [14] and DenseNet [17]. The VGGNet is originally

                                                       Ratio (%)                       64.8%
                                                        60
        designed for ImageNet classiﬁcation. For our experiment a                                 55.1%
                                                               49.0%      45.0%
        variation of the original VGGNet for CIFAR dataset is taken        40            34.8%
        from [36]. For ResNet, a 164-layer pre-activation ResNet        20    11.5%
        with bottleneck structure (ResNet-164) [15] is used. For         0
        DenseNet, we use a 40-layer DenseNet with growth rate 12             VGGNet   DenseNet-40  ResNet-164
        (DenseNet-40).                                Figure 3: Comparison of pruned models withlowertest errors on On ImageNet dataset, we adopt the 11-layer (8-conv +   CIFAR-10 than the original models. The blue and green bars are 3 FC) “VGG-A” network [31] model with batch normaliza-   parameter and FLOP ratios between pruned and original models.
        tion from [4]. We remove the dropout layers since we use
        relatively heavy data augmentation. To prune the neurons   mented by building a new narrower model and copying the
        in fully-connected layers, we treat them as convolutional   corresponding weights from the model trained with sparsity.
        channels with 1×1 spatial size.
          On MNIST dataset, we evaluate our method on the same   Fine-tuning.After the pruning we obtain a narrower and
        3-layer fully-connected network as in [35].              more compact model, which is then ﬁne-tuned. On CIFAR,
                                                   SVHN and MNIST datasets, the ﬁne-tuning uses the same
        4.3. Training, Pruning and Fine­tuning            optimization setting as in training. For ImageNet dataset,
                                                   due to time constraint, we ﬁne-tune the pruned VGG-A withNormal Training.We train all the networks normally from   a learning rate of 10 −3 for only 5 epochs.scratch as baselines. All the networks are trained using
        SGD. On CIFAR and SVHN datasets we train using mini-   4.4. Results batch size 64 for 160 and 20 epochs, respectively. The ini-
        tial learning rate is set to 0.1, and is divided by 10 at 50%   CIFAR and SVHNThe results on CIFAR and SVHN are
        and 75% of the total number of training epochs. On Im-   shown in Table 1. We mark all lowest test errors of a model
        ageNet and MNIST datasets, we train our models for 60   inboldface.
        and 30 epochs respectively, with a batch size of 256, and an   Parameter and FLOP reductions. The purpose of net-initial learning rate of 0.1 which is divided by 10 after 1/3   work slimming is to reduce the amount of computing re-and 2/3 fraction of training epochs. We use a weight de-   sources needed. The last row of each model has≥60%cay of10 −4 and a Nesterov momentum [33] of 0.9 without   channels pruned while still maintaining similar accuracy todampening. The weight initialization introduced by [13] is   the baseline. The parameter saving can be up to 10×. Theadopted. Our optimization settings closely follow the orig-   FLOP reductions are typically around50%. To highlightinal implementation at [10]. In all our experiments, we ini-   network slimming’s efﬁciency, we plot the resource sav-tialize all channel scaling factors to be 0.5, since this gives   ings in Figure 3. It can be observed that VGGNet has ahigher accuracy for the baseline models compared with de-   large amount of redundant parameters that can be pruned.fault setting (all initialized to be 1) from [10].            On ResNet-164 the parameter and FLOP savings are rel-
        Training with Sparsity.For CIFAR and SVHN datasets,   atively insigniﬁcant, we conjecture this is due to its “bot-
        when training with channel sparse regularization, the hyper-   tleneck” structure has already functioned as selecting chan-
        parameteerλ, which controls the tradeoff between empiri-   nels. Also, on CIFAR-100 the reduction rate is typically
        cal loss and sparsity, is determined by a grid search over   slightly lower than CIFAR-10 and SVHN, which is possi-
        10 −3 , 10 −4 , 10 −5 on CIFAR-10 validation set. For VG-   bly due to the fact that CIFAR-100 contains more classes.
        GNet we chooseλ=10 −4 and for ResNet and DenseNet   Regularization Effect.From Table 1, we can observe that,λ=10 −5 . For VGG-A on ImageNet, we setλ=10 −5 . All   on ResNet and DenseNet, typically when40%channels areother settings are kept the same as in normal training.       pruned, the ﬁne-tuned network can achieve a lower test er-
        Pruning.When we prune the channels of models trained   ror than the original models. For example, DenseNet-40
        with sparsity, a pruning threshold on the scaling factors   with 40% channels pruned achieve a test error of 5.19%
        needs to be determined. Unlike in [23] where different lay-   on CIFAR-10, which is almost 1% lower than the original
        ers are pruned by different ratios, we use a global pruning   model. We hypothesize this is due to the regularization ef-
        threshold for simplicity. The pruning threshold is deter-   fect of L1 sparsity on channels, which naturally provides
        mined by a percentile among all scaling factors , e.g., 40%   feature selection in intermediate layers of a network. We
        or 60% channels are pruned. The pruning process is imple-   will analyze this effect in the next section.



                                                 2741               VGG-A       Baseline   50% Pruned                 (a) Multi-pass Scheme on CIFAR-10
               Params       132.9M     23.2M           IterTrained Fine-tunedParams PrunedFLOPs Pruned
             Params Pruned       -       82.5%           1  6.38 6.51     66.7%     38.6%
               FLOPs       4.57×10 10   3.18×10 10          2  6.23 6.11     84.7%     52.7%
             FLOPs Pruned       -       30.4%           3  5.87 6.10     91.4%     63.1%
           Validation Error (%)    36.69      36.66            4  6.19 6.59     95.6%     77.2%
                                                      5  5.96 7.73     98.3%     88.7%
                  Table 2: Results on ImageNet.                 6  7.79 9.70     99.4%     95.7%

        Model     Test Error (%)Params Pruned  #Neurons               (b) Multi-pass Scheme on CIFAR-100
        Baseline      1.43        -     784-500-300-10       IterTrained Fine-tunedParams PrunedFLOPs Pruned
        Pruned [35]    1.53      83.5%   434-174-78-10       1  27.72 26.52    59.1%     30.9%
        Pruned (ours)   1.49      84.4%   784-100-60-10       2  26.03 26.52    79.2%     46.1%
                                                      3  26.49 29.08    89.8%     67.3%
                   Table 3: Results on MNIST.                 4  28.17 30.59    95.3%     83.0%
                                                      5  30.04 36.35    98.3%     93.5%
                                                      6  35.91 46.73    99.4%     97.7%
        ImageNet. The results for ImageNet dataset are summa-
        rized in Table 2. When 50% channels are pruned, the pa-   Table 4: Results for multi-pass scheme on CIFAR-10 and CIFAR-
        rameter saving is more than 5×, while the FLOP saving   100 datasets, using VGGNet. The baseline model has test errors of
        is only 30.4%. This is due to the fact that only 378 (out   6.34% and 26.74%. “Trained” and “Fine-tuned” columns denote
        of 2752) channels from all the computation-intensive con-   the test errors (%) of the model trained with sparsity, and the ﬁne-
                                                   tuned model after channel pruning, respectively. The parameter volutional layers are pruned, while 5094 neurons (out of   and FLOP pruned ratios correspond to the ﬁne-tuned model in that 8192) from the parameter-intensive fully-connected layers   row and the trained model in the next row. are pruned. It is worth noting that our method can achieve
        the savings with no accuracy loss on the 1000-class Im-   more compact models. On CIFAR-10, the trained modelageNet dataset, where other methods for efﬁcient CNNs   achieves the lowest test error in iteration 5. This model[2, 23, 35, 28] mostly report accuracy loss.              achieves 20×parameter reduction and 5×FLOP reduction,
        MNIST.On MNIST dataset, we compare our method with   while still achievinglowertest error. On CIFAR-100, after
        the Structured Sparsity Learning (SSL) method [35] in Ta-   iteration 3, the test error begins to increase. This is pos-
        ble 3. Despite our method is mainly designed to prune   sibly due to that it contains more classes than CIFAR-10,
        channels in convolutional layers, it also works well in prun-   so pruning channels too agressively will inevitably hurt the
        ing neurons in fully-connected layers. In this experiment,   performance. However, we can still prune near 90% param-
        we observe that pruning with a global threshold sometimes   eters and near 70% FLOPs without notable accuracy loss.
        completely removes a layer, thus we prune 80% of the neu-
        rons in each of the two intermediate layers. Our method   5. Analysis
        slightly outperforms [35], in that a slightly lower test error     There are two crucial hyper-parameters in network slim-is achieved while pruning more parameters.              ming, the pruned percentagetand the coefﬁcient of the
          We provide some additional experimental results in the   sparsity regularization termλ(see Equation 1). In this sec-
        supplementary materials, including (1) detailed structure of   tion, we analyze their effects in more detail.
        a compact VGGNet on CIFAR-10; (2) wall-clock time and   Effect of Pruned Percentage. Once we obtain a modelrun-time memory savings in practice. (3) comparison with   trained with sparsity regularization, we need to decide whata previous channel pruning method [23];                percentage of channels to prune from the model. If we
        4.5. Results for Multi­pass Scheme                prune too few channels, the resource saving can be very
                                                   limited. However, it could be destructive to the model if
          We employ the multi-pass scheme on CIFAR datasets   we prune too many channels, and it may not be possible to
        using VGGNet. Since there are no skip-connections, prun-   recover the accuracy by ﬁne-tuning. We train a DenseNet-
        ing away a whole layer will completely destroy the mod-   40 model withλ=10 −5 on CIFAR-10 to show the effect of
        els. Thus, besides setting the percentile threshold as 50%,   pruning a varying percentage of channels. The results are
        we also put a constraint that at each layer, at most 50% of   summarized in Figure 5.
        channels can be pruned.                             From Figure 5, it can be concluded that the classiﬁcation
          The test errors of models in each iteration are shown in   performance of the pruned or ﬁne-tuned models degrade
        Table 4. As the pruning process goes, we obtain more and   only when the pruning ratio surpasses a threshold. The ﬁne-



                                                 2742                         λ= 0                    λ= 10 −5                    λ= 10 −4
             400                       450                      2000
             350                       400
             300                       350                      1500
                                      300250

             Count                         250200                                               1000200150                       150
             100                       100                       500
              50                       50
               0                        0                        00.0  0.2  0.4  0.6  0.8      0.0  0.2  0.4  0.6  0.8      0.0  0.2  0.4  0.6  0.8
                                              Scaling factor value
        Figure 4: Distributions of scaling factors in a trained VGGNet under various degree of sparsity regularization (controlled by the parameter
        λ). With the increase ofλ, scaling factors become sparser.
            8.0                                         0Baseline
            7.5    Trained with Sparsity                          10 Pruned 7.0    Fine-tuned




                                                     Channel Index )
           %                                          20



           Test error ( 6.5
                                                      30 6.0
                                                      40 5.5
            5.0                                        50

            4.50  10 20 30 40 50 60 70 80 90            0     20     40     60     80
                      Pruned channels (%)                                   Epoch
        Figure 5: The effect of pruning varying percentages of channels,   Figure 6: Visulization of channel scaling factors’ change in scale
        from DenseNet-40 trained on CIFAR-10 withλ=10 −5 .          along the training process, taken from the 11th conv-layer in VG-
                                                   GNet trained on CIFAR-10. Brighter color corresponds to larger
                                                   value. The bright lines indicate the “selected” channels, the dark
        tuning process can typically compensate the possible accu-   lines indicate channels that can be pruned.
        racy loss caused by pruning. Only when the threshold goes
        beyond 80%, the test error of ﬁne-tuned model falls behind   progresses, some channels’ scaling factors become largerthe baseline model. Notably, when trained with sparsity,   (brighter) while others become smaller (darker).even without ﬁne-tuning, the model performs better than the
        original model. This is possibly due the the regularization   6. Conclusion effect of L1 sparsity on channel scaling factors.
                                                     We proposed the network slimming technique to learnChannel Sparsity Regularization.The purpose of the L1   more compact CNNs. It directly imposes sparsity-inducedsparsity term is to force many of the scaling factors to be   regularization on the scaling factors in batch normalizationnear zero. The parameterλin Equation 1 controls its signif-   layers, and unimportant channels can thus be automatically icance compared with the normal training loss. In Figure 4   identiﬁed during training and then pruned. On multiple we plot the distributions of scaling factors in the whole net-   datasets, we have shown that the proposed method is able towork with differentλvalues. For this experiment we use a   signiﬁcantly decrease the computational cost (up to 20×) ofVGGNet trained on CIFAR-10 dataset.                 state-of-the-art networks, with no accuracy loss. More im-
          It can be observed that with the increase ofλ, the scaling   portantly, the proposed method simultaneously reduces the
        factors are more and more concentrated near zero. When   model size, run-time memory, computing operations while
        λ=0, i.e., there’s no sparsity regularization, the distribution   introducing minimum overhead to the training process, and
        is relatively ﬂat. Whenλ=10 −4 , almost all scaling factors   the resulting models require no special libraries/hardware
        fall into a small region near zero. This process can be seen   for efﬁcient inference.
        as a feature selection happening in intermediate layers of
        deep networks, where only channels with non-negligible   Acknowledgements. Gao Huang is supported by the In-
        scaling factors are chosen. We further visualize this pro-   ternational Postdoctoral Exchange Fellowship Program of
        cess by a heatmap. Figure 6 shows the magnitude of scaling   China Postdoctoral Council (No.20150015). Changshui
        factors from one layer in VGGNet, along the training pro-   Zhang is supported by NSFC and DFG joint project NSFC
        cess. Each channel starts with equal weights; as the training   61621136008/DFG TRR-169.



                                                 2743         References                                     [20] J. Jin, Z. Yan, K. Fu, N. Jiang, and C. Zhang. Neural network
                                                            architecture optimization through submodularity and super- [1] B. Baker, O. Gupta, N. Naik, and R. Raskar. Designing neu-       modularity.arXiv preprint arXiv:1609.00074, 2016. ral network architectures using reinforcement learning. In    [21] A. Krizhevsky and G. Hinton. Learning multiple layers of ICLR, 2017.                                       features from tiny images. InTech Report, 2009. [2] S. Changpinyo, M. Sandler, and A. Zhmoginov. The power    [22] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenetof sparsity in convolutional neural networks.arXiv preprint       classiﬁcation with deep convolutional neural networks. In arXiv:1702.06257, 2017.                               NIPS, pages 1097–1105, 2012. [3] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and    [23] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Y. Chen. Compressing neural networks with the hashing       Graf. Pruning ﬁlters for efﬁcient convnets. arXiv preprint trick. InICML, 2015.                                 arXiv:1608.08710, 2016.
          [4] S. Chintala. Training an object classiﬁer in torch-7 on    [24] M. Lin, Q. Chen, and S. Yan. Network in network. InICLR,multiple gpus over imagenet. https://github.com/       2014.soumith/imagenet-multiGPU.torch.            [25] B. Liu, M. Wang, H. Foroosh, M. Tappen, and M. Pensky.
          [5] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A       Sparse convolutional neural networks. InProceedings of the
            matlab-like environment for machine learning. InBigLearn,       IEEE Conference on Computer Vision and Pattern Recogni-
            NIPS Workshop, number EPFL-CONF-192376, 2011.            tion, pages 806–814, 2015.
          [6] M. Courbariaux and Y. Bengio. Binarynet: Training deep    [26] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
            neural networks with weights and activations constrained to+       networks for semantic segmentation. InCVPR, pages 3431–
            1 or-1.arXiv preprint arXiv:1602.02830, 2016.                3440, 2015.
          [7] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fer-    [27] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
            gus. Exploiting linear structure within convolutional net-       Ng. Reading digits in natural images with unsupervised fea-
            works for efﬁcient evaluation. InNIPS, 2014.                 ture learning, 2011. InNIPS Workshop on Deep Learning
          [8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-       and Unsupervised Feature Learning, 2011.
            ture hierarchies for accurate object detection and semantic    [28] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-
            segmentation. InCVPR, pages 580–587, 2014.                net: Imagenet classiﬁcation using binary convolutional neu-
          [9] I. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and       ral networks. InECCV, 2016.
            Y. Bengio. Maxout networks. InICML, 2013.             [29] S. Scardapane, D. Comminiello, A. Hussain, and A. Uncini.
         [10] S. Gross and M. Wilber. Training and investigating residual       Group sparse regularization for deep neural networks.arXiv
            nets. https://github.com/szagoruyko/cifar.       preprint arXiv:1607.00485, 2016.
            torch.                                      [30] M. Schmidt, G. Fung, and R. Rosales. Fast optimization
         [11] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-       methods for l1 regularization: A comparative study and two
            pressing deep neural network with pruning, trained quanti-       new approaches. InECML, pages 286–297, 2007.
            zation and huffman coding. InICLR, 2016.               [31] K. Simonyan and A. Zisserman. Very deep convolutional
         [12] S. Han, J. Pool, J. Tran, and W. Dally. Learning both weights       networks for large-scale image recognition. InICLR, 2015.
            and connections for efﬁcient neural network. InNIPS, pages    [32] S. Srinivas, A. Subramanya, and R. V. Babu. Training sparse
            1135–1143, 2015.                                   neural networks.CoRR, abs/1611.06694, 2016.
         [13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into    [33] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the
            rectiﬁers: Surpassing human-level performance on imagenet       importance of initialization and momentum in deep learning.
            classiﬁcation. InICCV, 2015.                            InICML, 2013.
         [14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning    [34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
            for image recognition. InCVPR, 2016.                      D. Anguelov, D. Erhan, et al. Going deeper with convolu-
                                                            tions. InCVPR, pages 1–9, 2015.[15] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in    [35] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learningdeep residual networks. InECCV, pages 630–645. Springer,       structured sparsity in deep neural networks. InNIPS, 2016.2016.                                        [36] S. Zagoruyko. 92.5% on cifar-10 in torch. https://[16] G. Huang, D. Chen, T. Li, F. Wu, L. van der Maaten, and       github.com/szagoruyko/cifar.torch.K. Q. Weinberger. Multi-scale dense convolutional networks
            for efﬁcient prediction. arXiv preprint arXiv:1703.09844,    [37] H. Zhou, J. M. Alvarez, and F. Porikli. Less is more: Towards
            2017.                                            compact cnns. InECCV, 2016.
                                                         [38] B. Zoph and Q. V. Le. Neural architecture search with rein-[17] G. Huang, Z. Liu, K. Q. Weinberger, and L. van der Maaten.       forcement learning. InICLR, 2017.Densely connected convolutional networks. InCVPR, 2017.
         [18] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger.
            Deep networks with stochastic depth. InECCV, 2016.
         [19] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
            deep network training by reducing internal covariate shift.
            arXiv preprint arXiv:1502.03167, 2015.




                                                       2744