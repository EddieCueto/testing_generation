                 Channel Pruning for Accelerating Very Deep Neural Networks


                     Yihui He *               Xiangyu Zhang              Jian Sun
              Xi’an Jiaotong University          Megvii Inc.               Megvii Inc.
                Xi’an, 710049, China       Beijing, 100190, China     Beijing, 100190, China
              heyihui@stu.xjtu.edu.cn    zhangxiangyu@megvii.com      sunjian@megvii.com



                        Abstract                         W1

          In this paper, we introduce a new channel pruning      number of  channels
                                                                                   nonlinear method to accelerate very deep convolutional neural net-
        works. Given a trained CNN model, we propose an it-
        erative two-step algorithm to effectively prune each layer,         W2
        by a LASSO regression based channel selection and least                                    nonlinear
        square reconstruction. We further generalize this algorithm
        to multi-layer and multi-branch cases. Our method re-        W3
        duces the accumulated error and enhance the compatibility
        with various architectures. Our pruned VGG-16 achieves         (a)                           (b)                        (c)                       (d)
        the state-of-the-art results by5×speed-up along with only   Figure 1. Structured simpliﬁcation methods that accelerate CNNs: 0.3% increase of error. More importantly, our method is   (a) a network with 3 conv layers. (b) sparse connection deacti-
        able to accelerate modern networks like ResNet, Xception   vates some connections between channels. (c) tensor factorization
        and suffers only 1.4%, 1.0% accuracy loss under2×speed-   factorizes a convolutional layer into several pieces. (d) channel
        up respectively, which is signiﬁcant.                   pruning reduces number of channels in each layer (focus of this
                                                   paper).

        1. Introduction                              a network into thinner one, as shown in Fig.1(d). It is efﬁ-
          Recent CNN acceleration works fall into three cate-   cient on both CPU and GPU because no special implemen-
        gories: optimized implementation (e.g., FFT [47]), quan-   tation is required.
        tization (e.g., BinaryNet [8]), and structured simpliﬁcation     Pruning channels is simple but challenging because re-
        that convert a CNN into compact one [22]. This work fo-   moving channels in one layer might dramatically change
        cuses on the last one.                             the input of the following layer. Recently,training-based
          Structured simpliﬁcation mainly involves: tensor fac-   channel pruning works [1,48] have focused on imposing
        torization [22], sparse connection [17], and channel prun-   sparse constrain on weights during training, which could
        ing [48]. Tensor factorization factorizes a convolutional   adaptively determine hyper-parameters. However, training
        layer into several efﬁcient ones (Fig.1(c)). However, fea-   from scratch is very costly and results for very deep CNNs
        ture map width (number of channels) could not be reduced,   on ImageNet have been rarely reported.Inference-timeat-
        which makes it difﬁcult to decompose1×1convolutional   tempts [31,3] have focused on analysis of the importance
        layer favored by modern networks (e.g., GoogleNet [45],   of individual weight. The reported speed-up ratio is very
        ResNet [18], Xception [7]). This type of method also intro-   limited.
        duces extra computation overhead. Sparse connection deac-     In this paper, we propose a new inference-time approach
        tivates connections between neurons or channels (Fig.1(b)).   for channel pruning, utilizing redundancy inter channels.
        Though it is able to achieves high theoretical speed-up ratio,   Inspired by tensor factorization improvement by feature
        the sparse convolutional layers have an ”irregular” shape   maps reconstruction [52], instead of analyzing ﬁlter weights
        which is not implementation friendly. In contrast, channel   [22,31], we fully exploits redundancy within feature maps.
        pruning directly reduces feature map width, which shrinks   Speciﬁcally, given a trained CNN model, pruning each layer
                                                   is achieved by minimizing reconstruction error on its output
          * This work was done when Yihui He was an intern at Megvii Inc.      feature maps, as showned in Fig.2. We solve this mini-



                                                 1389                A                                                B                                                                      C           maps. There are several training-based approaches. [1,48]
                                 W                 regularize networks to improve accuracy. Channel-wise
                                                   SSL [48] reaches high compression ratio for ﬁrst few conv
                                                   layers of LeNet [30] and AlexNet [26]. However,training- kh kc  w              basedapproaches are more costly, and the effectiveness for
                         c           n             very deep networks on large datasets is rarely exploited. nonlinear                 nonlinear
        Figure 2. Channel pruning for accelerating a convolutional layer.     Inference-time channel pruning is challenging, as re-
        We aim to reduce the width of feature map B, while minimizing   ported by previous works [2,39]. Some works [44,34,19]
        the reconstruction error on feature map C. Our optimization algo-   focus on model size compression, which mainly operate the
        rithm (Sec. 3.1) performs within the dotted box, which does not   fully connected layers. Data-free approaches [31,3] results
        involve nonlinearity. This ﬁgure illustrates the situation that two   for speed-up ratio (e.g.,5×) have not been reported, and
        channels are pruned for feature map B. Thus corresponding chan-   requires long retraining procedure. [3] select channels via
        nels of ﬁltersWcan be removed. Furthermore, even though not   over 100 random trials, however it need long time to evalu- directly optimized by our algorithm, the corresponding ﬁlters in   ate each trial on a deep network, which makes it infeasible the previous layer can also be removed (marked by dotted ﬁlters).   to work on very deep models and large datasets. [31] is even c,n: number of channels for feature maps B and C,kh ×kw :   worse than naive solution from our observation sometimes kernel size.                                    (Sec.4.1.1).

        mization problem by two alternative steps: channels selec-   3. Approach
        tion and feature map reconstruction. In one step, we ﬁgure     In this section, we ﬁrst propose a channel pruning al-out the most representative channels, and prune redundant   gorithm for a single layer, then generalize this approach toones, based on LASSO regression. In the other step, we   multiple layers or the whole model. Furthermore, we dis-reconstruct the outputs with remaining channels with linear   cuss variants of our approach for multi-branch networks.least squares. We alternatively take two steps. Further, we
        approximate the network layer-by-layer, with accumulated   3.1. Formulation
        error accounted. We also discuss methodologies to prune
        multi-branch networks (e.g., ResNet [18], Xception [7]).       Fig.2illustrates our channel pruning algorithm for a sin-
          For VGG-16, we achieve4×acceleration, with only   gle convolutional layer. We aim to reduce the width of
        1.0%increase of top-5 error. Combined with tensor factor-   feature map B, while maintaining outputs in feature map
        ization, we reach5×acceleration but merely suffer0.3%   C. Once channels are pruned, we can remove correspond-
        increase of error, which outperforms previous state-of-the-   ing channels of the ﬁlters that take these channels as in-
        arts. We further speed up ResNet-50 and Xception-50 by   put. Also, ﬁlters that produce these channels can also be
        2×with only1.4%, 1.0%accuracy loss respectively.       removed. It is clear that channel pruning involves two key
                                                   points. The ﬁrst is channel selection, since we need to select
        2. Related Work                             most representative channels to maintain as much informa-
                                                   tion. The second is reconstruction. We need to reconstruct
          There has been a signiﬁcant amount of work on acceler-   the following feature maps using the selected channels.
        ating CNNs. Many of them fall into three categories: opti-     Motivated by this, we propose an iterative two-step al-
        mized implementation [4], quantization [40], and structured   gorithm. In one step, we aim to select most representative
        simpliﬁcation [22].                              channels. Since an exhaustive search is infeasible even for
          Optimized implementation based methods [35,47,27,4]   tiny networks, we come up with a LASSO regression based
        accelerate convolution, with special convolution algorithms   method to ﬁgure out representative channels and prune re-
        like FFT [47]. Quantization [8,40] reduces ﬂoating point   dundant ones. In the other step, we reconstruct the outputs
        computational complexity.                         with remaining channels with linear least squares. We alter-
          Sparse connection eliminates connections between neu-   natively take two steps.
        rons [17,32,29,15,14]. [51] prunes connections based on     Formally, to prune a feature map withcchannels, we
        weights magnitude. [16] could accelerate fully connected   consider applyingn×c×kh ×kw convolutional ﬁltersWon
        layers up to50×. However, in practice, the actual speed-up   N×c×kh ×kw input volumesXsampled from this feature
        maybe very related to implementation.                 map, which producesN×noutput matrixY. Here,Nis
          Tensor factorization [22,28,13,24] decompose weights   the number of samples,nis the number of output channels,
        into several pieces. [50,10,12] accelerate fully connected   andkh ,k w are the kernel size. For simple representation,
        layers with truncated SVD. [52] factorize a layer into3×3   bias term is not included in our formulation. To prune the
        and1×1combination, driven by feature map redundancy.    input channels fromcto desiredc′ (0≤c′ ≤c), while
          Channel pruning removes redundant channels on feature   minimizing reconstruction error, we formulate our problem



                                                 1390        as follow:                                    penalty, and β  =c. We gradually increaseλ. For each 0                           change ofλ, we iterate these two steps until β  is stable.
                      1               2                                            0      c                    After β  ≤c′ satisﬁes, we obtain the ﬁnal solutionWarg min    Y−   β                         0i Xi W⊤  i              from{ββ,W 2N                    (1)         i Wi }. In practice, we found that the two steps it- i=1       F           eration is time consuming. So we apply (i) multiple times,subject to β  ≤c′
                         0                         until β  ≤c′ satisﬁes. Then apply (ii) just once, to obtain 0
           ·  is Frobenius norm.X                      the ﬁnal result. From our observation, this result is compa-
            F               i isN×kh kw matrix sliced
        fromith channel of input volumesX,i= 1,...,c.W     rable with two steps iteration’s. Therefore, in the following i is
        n×k                                       experiments, we adopt this approach for efﬁciency. h kw ﬁlter weights sliced fromith channel ofW.βis
        coefﬁcient vector of lengthcfor channel selection, andβ      Discussion: Some recent works [48,1,17] (though train- i
        isith entry ofβ. Notice that, ifβ                     ing based) also introduceℓ1 -norm or LASSO. However, we i = 0,Xi will be no longer
        useful, which could be safely pruned from feature map.W    must emphasis that we use different formulations. Many of i
        could also be removed.                           them introduced sparsity regularization into training loss,
        Optimization                                 instead of explicitly solving LASSO. Other work [1] solved
        Solving thisℓ                                  LASSO, while feature maps or data were not considered 0 minimization problem in Eqn.1is NP-hard.
        Therefore, we relax theℓ                          during optimization. Because of these differences, our ap- 0 toℓ1 regularization:            proach could be applied at inference time.              
                   1       c        2
            arg min    Y−   β                      3.2. Whole Model Pruning i Xi W⊤  
                                 i   +λ β 1β,W 2N                        (2) i=1       F                Inspired by [52], we apply our approach layer by layersubject to β  ≤c′ ,∀i W  = 1 0        iF                  sequentially. For each layer, we obtain input volumes from
                                                   the current input feature map, and output volumes from theλis a penalty coefﬁcient. By increasingλ, there will be   output feature map of the un-pruned model. This could bemore zero terms inβand one can get higher speed-up ratio.   formalized as:We also add a constrain∀i Wi   = 1to this formulation, F which avoids trivial solution.                                                      
          Now we solve this problem in two folds. First, we ﬁxW,                 1       c        2
                                                           arg min    Y′ −   βsolveβfor channel selection. Second, we ﬁxβ, solveWto                            i Xi W⊤  i  
                                                            β,W 2N                    (5)
        reconstruct error.                                                    i=1       F
          (i) The subproblem ofβ. In this case,Wis ﬁxed. We           subject to β  ≤c′
                                                                    0
        solveβfor channel selection. This problem can be solved     Different from Eqn.1,Yis replaced byY′ , which is fromby LASSO regression [46,5], which is widely used for   feature map of the original model. Therefore, the accumu-model selection.                                lated error could be accounted during sequential pruning.                    2      c      βˆLASSO           1(λ) = argmin    Y−   β    +λ β      3.3. Pruning Multi­Branch Networks
                     β  2N       i Zi         1
                                i=1    F             The whole model pruning discussed above is enough for
         subject to β  ≤c′
                  0                                single-branch networks like LeNet [30], AlexNet [26] and(3)   VGG Nets [43]. However, it is insufﬁcient for multi-branch HereZi = X i W⊤ i (sizeN×n). We will ignoreith channels   networks like GoogLeNet [ 45] and ResNet [18]. We mainlyifβi = 0.                                    focus on pruning the widely used residual structure (e.g.,(ii) The subproblem ofW. In this case,βis ﬁxed. We   ResNet [18], Xception [7]). Given a residual block shownutilize the selected channels to minimize reconstruction er-   in Fig.3(left), the input bifurcates into shortcut and residualror. We can ﬁnd optimized solution by least squares:        branch. On the residual branch, there are several convolu-
                                                  tional layers (e.g., 3 convolutional layers which have spatialarg min Y−X′ (W ′ )⊤  2        (4) F              size of1×1,3×3,1×1, Fig.3, left). Other layers ex- W′                            cept the ﬁrst and last layer can be pruned as is described
        HereX′ = [β1 X1 β2 X2 ... β i Xi ... β c Xc ](size   previously. For the ﬁrst layer, the challenge is that the large
        N×ck h kw ). W′ isn×ck h kw reshapedW,W′ =   input feature map width (for ResNet, 4 times of its output)
        [W 1 W2 ...Wi ...Wc ]. After obtained resultW′ , it is re-   can’t be easily pruned, since it’s shared with shortcut. For
        shaped back toW. Then we assignβi ←βi  Wi   ,W      the last layer, accumulated error from the shortcut is hard to F  i ←
        Wi / Wi   . Constrain∀i W                      be recovered, since there’s no parameter on the shortcut. To F            i   = 1satisﬁes. F We alternatively optimize (i) and (ii). In the beginning,   address these challenges, we propose several variants of our
        Wis initialized from the trained model,λ= 0, namely no   approach as follows.



                                                 1391                                    c              ers, which need special library implementation support. We
                      Input (c) sampled (c')  0              do not adopt it in the following experiments. c             0      0 
             0
                          channel     sampler
                          sampler 1x1,c                   c'0               4. Experiment 1
            c                       1x1 1   relu                c' 3x3,c                   1   relu           We evaluation our approach for the popular VGG Nets 2
            c                       3x3 2   relu                                [43], ResNet [18], Xception [7] on ImageNet [9], CIFAR- c'1x1                    2   relu         10 [25] and PASCAL VOC 2007 [11]. 1x1                For Batch Normalization [21], we ﬁrst merge it into con- Y2   Y          volutional weights, which do not affect the outputs of the Y+Y    1
                                   1 2              networks. So that each convolutional layer is followed by
        Figure 3. Illustration of multi-branch enhancement for residual   ReLU [36]. We use Caffe [23] for deep network evalua-
        block.Left: original residual block.Right: pruned residual block   tion, and scikit-learn [38] for solvers implementation. For
        with enhancement,cx denotes the feature map width. Input chan-   channel pruning, we found that it is enough to extract 5000 nels of the ﬁrst convolutional layer are sampled, so that the large   images, and 10 samples per image. On ImageNet, we eval- input feature map width could be reduced. As for the last layer,   uate the top-5 accuracy with single view. Images are re- rather than approximateY2 , we try to approximateY1 + Y 2 di-   sized such that the shorter side is 256. The testing is on rectly (Sec.3.3Last layer of residual branch).               center crop of224×224pixels. We could gain more per-
                                                   formance with ﬁne-tuning. We use a batch size of 128 and
                                                   learning rate1e−5 . We ﬁne-tune our pruned models for 10Last layer of residual branch: Shown in Fig.3, the   epoches. The augmentation for ﬁne-tuning is random cropoutput layer of a residual block consists of two inputs: fea-   of224×224and mirror.ture mapY1 andY2 from the shortcut and residual branch.
        We aim to recoverY1 + Y 2 for this block. Here,Y1 ,Y2   4.1. Experiments with VGG­16 are the original feature maps before pruning.Y2 could be
        approximated as in Eqn.1. However, shortcut branch is     VGG-16 [43] is a 16 layers single path convolutional
        parameter-free, thenY                            neural network, with 13 convolutional layers. It is widely 1 could not be recovered directly. To
        compensate this error, the optimization goal of the last layer   used in recognition, detection and segmentation,etc. Single
        is changed fromY                               view top-5 accuracy for VGG-16 is 89.9% 1 .2 toY1 −Y′ +Y, which does not change 1  2
        our optimization. Here,Y′ is the current feature map after 1 previous layers pruned. When pruning, volumes should be   4.1.1 Single Layer Pruning
        sampled correspondingly from these two branches.         In this subsection, we evaluate single layer acceleration per-First layer of residual branch: Illustrated in   formance using our algorithm in Sec.3.1. For better under-Fig.3(left), the input feature map of the residual block   standing, we compare our algorithm with two naive chan-could not be pruned, since it is also shared with the short-   nel selection strategies.ﬁrst kselects the ﬁrstkchannels.cut branch. In this condition, we could performfeature   max responseselects channels based on corresponding ﬁl-map samplingbefore the ﬁrst convolution to save compu-   ters that have high absolute weights sum [31]. For fair com-tation. We still apply our algorithm as Eqn.1. Differently,   parison, we obtain the feature map indexes selected by eachwe sample the selected channels on the shared feature maps   of them, then perform reconstruction (Sec. 3.1(ii)). We to construct a new input for the later convolution, shown   hope that this could demonstrate the importance of channelin Fig.3(right). Computational cost for this operation could   selection. Performance is measured by increase of error af-be ignored. More importantly, after introducingfeature map   ter a certain layer is pruned without ﬁne-tuning, shown insampling, the convolution is still ”regular”.              Fig.4.Filter-wise pruningis another option for the ﬁrst con-     As expected, error increases as speed-up ratio increases.volution on the residual branch. Since the input channels   Our approach is consistently better than other approaches inof parameter-free shortcut branch could not be pruned, we   different convolutional layers under different speed-up ra-apply our Eqn.1to each ﬁlter independently (each ﬁl-   tio. Unexpectedly, sometimesmax responseis even worseter chooses its own representative input channels). Under   thanﬁrst k. We argue thatmax responseignores correla-single layer acceleration,ﬁlter-wise pruningis more accu-   tions between different ﬁlters. Filters with large absoluterate than our original one. From our experiments, it im-   weight may have strong correlation. Thus selection based proves 0.5% top-5 accuracy for2×ResNet-50 (applied on   on ﬁlter weights is less meaningful. Correlation on featurethe ﬁrst layer of each residual branch) without ﬁne-tuning.   maps is worth exploiting. We can ﬁnd that channel selectionHowever, after ﬁne-tuning, there’s no noticeable improve-
        ment. In addition, it outputs ”irregular” convolutional lay-     1 http://www.vlfeat.org/matconvnet/pretrained/



                                                 1392                          conv1_1                 conv2_1                 conv3_1 5
                             first k                  first k                  first k
                             max response              max response              max response 4          ours                   ours                   ours





                 increase of error (%) 3

                  2

                  1

                  0

                          conv3_2                 conv4_1                 conv4_2 5
                             first k                  first k             first k
                             max response              max response        max response 4          ours                   ours             ours





                 increase of error (%) 3

                  2

                  1

                  01.0 1.5 2.0 2.5 3.0 3.5 4.0  1.0 1.5 2.0 2.5 3.0 3.5 4.0  1.0 1.5 2.0 2.5 3.0 3.5 4.0
                         speed-up ratio               speed-up ratio               speed-up ratio
        Figure 4. Single layer performance analysis under different speed-up ratios (without ﬁne-tuning), measured by increase of error. To verify
        the importance of channel selection refered in Sec.3.1, we considered two naive baselines.ﬁrst kselects the ﬁrstkfeature maps.max
        responseselects channels based on absolute sum of corresponding weights ﬁlter [31]. Our approach is consistently better (smaller is
        better).


            Increase of top-5 error (1-view, baseline 89.9%)       periments above, we pruning more aggressive for shal-
                 Solution          2×  4×  5×    lower layers. Remaining channels ratios for shallow lay-
         Jaderberget al. [22] ([52]’s impl.)   -   9.7  29.7    ers (conv1_xtoconv3_x) and deep layers (conv4_x)
                Asym. [52]         0.28  3.84   -     is1 : 1.5.conv5_xare not pruned, since they only con-
              Filter pruning [31]                        tribute 9% computation in total and are not redundant.0.8  8.6  14.6(ﬁne-tuned, our impl.)                         After ﬁne-tuning, we could reach2×speed-up without
            Ours (without ﬁne-tune)     2.7  7.9  22.0    losing accuracy. Under4×, we only suffers 1.0% drops.
              Ours (ﬁne-tuned)        0   1.0  1.7    Consistent with single layer analysis, our approach outper-
        Table 1. Accelerating the VGG-16 model [43] using a speedup   forms previous channel pruning approach (Liet al. [31]) by
        ratio of2×,4×, or5×(smaller is better).                 large margin. This is because we fully exploits channel re-
                                                   dundancy within feature maps. Compared with tensor fac-
        affects reconstruction error a lot. Therefore, it is important   torization algorithms, our approach is better than Jaderberg
        for channel pruning.                             et al. [22], without ﬁne-tuning. Though worse than Asym.
          Also notice that channel pruning gradually becomes   [52], our combined model outperforms its combined Asym.
        hard, from shallower to deeper layers. It indicates that shal-   3D (Table2). This may indicate that channel pruning is
        lower layers have much more redundancy, which is consis-   more challenging than tensor factorization, since removing
        tent with [52]. We could prune more aggressively on shal-   channels in one layer might dramatically change the input
        lower layers in whole model acceleration.               of the following layer. However, channel pruning keeps the
                                                   original model architecture, do not introduce additional lay-
                                                   ers, and the absolute speed-up ratio on GPU is much higher4.1.2 Whole Model Pruning                      (Table 3).
        Shown in Table1, whole model acceleration results under     Since our approach exploits a new cardinality, we further
        2×,4×,5×are demonstrated. We adopt whole model   combine our channel pruning with spatial factorization [22]
        pruning proposed in Sec.3.2. Guided by single layer ex-   and channel factorization [52]. Demonstrated in Table2,



                                                 1393               Increase of top-5 error (1-view, 89.9%)          scratch. This coincides with architecture design researches
                     Solution        4×  5×          [20,1] that the model could be easier to train if there are
                  Asym. 3D [52]      0.9  2.0          more channels in shallower layers. However, channel prun-
              Asym. 3D (ﬁne-tuned) [52]  0.3  1.0          ing favors shallower layers.
                     Our 3C        0.7  1.3            For from scratch (uniformed), the ﬁlters in each layers
                 Our 3C (ﬁne-tuned)    0.0  0.3          is reduced by half (eg. reduceconv1_1from 64 to 32).
        Table 2. Performance of combined methods on the VGG-16 model   We can observe that normal setting networks of the same
        [43] using a speed-up ratio of4×or5×. Our 3C solution outper-   complexity couldn’t reach same accuracy either. This con-
        forms previous approaches (smaller is better).               solidates our idea that there’s much redundancy in networks
                                                   while training. However, redundancy can be opt out at
                                                   inference-time. This maybe an advantage of inference-timeour 3 cardinalities acceleration (spatial, channel factoriza-   acceleration approaches over training-based approaches.tion, and channel pruning, denoted by 3C) outperforms pre-     Notice that there’s a 0.6% gap between the from scratchvious state-of-the-arts. Asym. 3D [52] (spatial and chan-   model and uniformed one, which indicates that there’s roomnel factorization), factorizes a convolutional layer to three   for model exploration. Adopting our approach is muchparts:1×3,3×1,1×1.                         faster than training a model from scratch, even for a thin-We apply spatial factorization, channel factorization, and   ner one. Further researches could alleviate our approach to our channel pruning together sequentially layer-by-layer.   do thin model exploring.We ﬁne-tune the accelerated models for 20 epoches, since
        they are 3 times deeper than the original ones. After ﬁne-
        tuning, our4×model suffers no degradation. Clearly, a   4.1.5 Acceleration for Detection
        combination of different acceleration techniques is better   VGG-16 is popular among object detection tasks [42,41,than any single one. This indicates that a model is redun-   33]. We evaluate transfer learning ability of our2×/4×dant in each cardinality.                           pruned VGG-16, for Faster R-CNN [42] object detections.
                                                   PASCAL VOC 2007 object detection benchmark [11] con-
        4.1.3 Comparisons of Absolute Performance          tains 5k trainval images and 5k test images. The per-
                                                   formance is evaluated by mean Average Precision (mAP).We further evaluate absolute performance of acceleration   In our experiments, we ﬁrst perform channel pruning foron GPU. Results in Table3are obtained under Caffe [23],   VGG-16 on the ImageNet. Then we use the pruned modelCUDA8 [37] and cuDNN5 [6], with a mini-batch of 32   as the pre-trained model for Faster R-CNN.on a GPU (GeForce GTX TITAN X). Results are averaged     The actual running time of Faster R-CNN is 220ms / im-from 50 runs. Tensor factorization approaches decompose   age. The convolutional layers contributes about 64%. Weweights into too many pieces, which heavily increase over-   got actual time of 94ms for4×acceleration. From Table5,head. They could not gain much absolute speed-up. Though   we observe 0.4% mAP drops of our2×model, which is notour approach also encountered performance decadence, it   harmful for practice consideration.generalizes better on GPU than other approaches. Our re-
        sults for tensor factorization differ from previous research   4.2. Experiments with Residual Architecture Nets
        [52,22], maybe because current library and hardware pre-     For Multi-path networks [45,18,7], we further explorefer single large convolution instead of several small ones.     the popular ResNet [18] and latest Xception [7], on Ima-
                                                   geNet and CIFAR-10. Pruning residual architecture nets is
        4.1.4 Comparisons with Training from Scratch        more challenging. These networks are designed for both ef-
                                                   ﬁciency and high accuracy. Tensor factorization algorithmsThough training a compact model from scratch is time-   [52,22] have difﬁcult to accelerate these model. Spatially,consuming (usually 120 epoches), it worths comparing our   1×1convolution is favored, which could hardly be factor-approach and from scratch counterparts. To be fair, we eval-   ized.uated both from scratch counterpart, and normal setting net-
        work that has the same computational complexity and same   4.2.1 ResNet Pruningarchitecture.
          Shown in Table4, we observed that it’s difﬁcult for   ResNet complexity uniformly drops on each residual block.
        from scratch counterparts to reach competitive accuracy.   Guided by single layer experiments (Sec. 4.1.1), we still
        our model outperforms from scratch one. Our approach   prefer reducing shallower layers heavier than deeper ones.
        successfully picks out informative channels and constructs     Following similar setting as Filter pruning [31], we
        highly compact models. We can safely draw the conclu-   keep 70% channels for sensitive residual blocks (res5
        sion that the same model is difﬁcult to be obtained from   and blocks close to the position where spatial size



                                                 1394                       Model             Solution          Increased err.  GPU time/ms
                       VGG-16              -                 0        8.144
                                Jaderberget al. [22] ([52]’s impl.)     9.7     8.051(1.01×)
                                        Asym. [52]            3.8     5.244(1.55×)
                     VGG-16 (4×)        Asym. 3D [52]           0.9     8.503(0.96×)
                                  Asym. 3D (ﬁne-tuned) [52]       0.3     8.503(0.96×)
                                      Ours (ﬁne-tuned)           1.0     3.264 (2.50×)
        Table 3. GPU acceleration comparison. We measure forward-pass time per image. Our approach generalizes well on GPU (smaller is
        better).


           Original (acc. 89.9%)   Top-5 err.  Increased err.                Solution         Increased err.
             From scratch        11.9       1.8            Filter pruning [31] (our impl.)     92.8
         From scratch (uniformed)   12.5       2.4                Filter pruning [31]         4.3Ours          18.0       7.9               (ﬁne-tuned, our impl.)
            Ours (ﬁne-tuned)      11.1       1.0                     Ours             2.9
        Table 4. Comparisons with training from scratch, under4×accel-            Ours (ﬁne-tuned)         1.0
        eration. Our ﬁne-tuned model outperforms scratch trained coun-   Table 7. Comparisons for Xception-50, under2×acceleration ra-
        terparts (smaller is better).                           tio. The baseline network’s top-5 accuracy is 92.8%. Our ap-
                                                   proach outperforms previous approaches. Most structured sim-
                                                   pliﬁcation methods are not effective on Xception architecture
                  Speedup  mAP  ∆mAP              (smaller is better).
                  Baseline  68.7    -
                    2×    68.3   0.4
                    4×    66.9   1.8               4.2.2 Xception Pruning
          Table 5.2×,4×acceleration for Faster R-CNN detection.
                                                   Since computational complexity becomes important in
                                                   model design, separable convolution has been payed muchSolution      Increased err.          attention [49,7]. Xception [7] is already spatially optimizedOurs           8.0             and tensor factorization on1×1convolutional layer is de-Ours           4.0             structive. Thanks to our approach, it could still be acceler-(enhanced)                         ated with graceful degradation. For the ease of comparison,Ours           1.4             we adopt Xception convolution on ResNet-50, denoted by(enhanced, ﬁne-tuned)                     Xception-50. Based on ResNet-50, we swap all convolu- Table 6.2×acceleration for ResNet-50 on ImageNet, the base-   tional layers with spatial conv blocks. To keep the same line network’s top-5 accuracy is 92.2% (one view). We improve   computational complexity, we increase the input channels performance with multi-branch enhancement (Sec.3.3,smaller is   of allbranch2blayers by2×. The baseline Xception- better).                                      50 has a top-5 accuracy of 92.8% and complexity of 4450
                                                   MFLOPs.
                                                     We apply multi-branch variants of our approach as de-change, e.g. res3a,res3d). As for other blocks,   scribed in Sec.3.3, and adopt the same pruning ratio settingwe keep 30% channels. With multi-branch enhance-   as ResNet in previous section. Maybe because of Xcep-ment, we prunebranch2amore aggressively within   tion block is unstable, Batch Normalization layers must beeach residual block. The remaining channels ratios for   maintained during pruning. Otherwise it becomes nontrivialbranch2a,branch2b,branch2cis2 : 4 : 3(e.g.,   to ﬁne-tune the pruned model.Given 30%, we keep 40%, 80%, 60% respectively).          Shown in Table7, after ﬁne-tuning, we only suffer1.0%
          We evaluate performance of multi-branch variants of our   increase of error under2×. Filter pruning [31] could also
        approach (Sec. 3.3). From Table6, we improve 4.0%   apply on Xception, though it is designed for small speed-
        with our multi-branch enhancement. This is because we   up ratio. Without ﬁne-tuning, top-5 error is 100%. After
        accounted the accumulated error from shortcut connection   training 20 epochs which is like training from scratch, in-
        which could broadcast to every layer after it. And the large   creased error reach 4.3%. Our results for Xception-50 are
        input feature map width at the entry of each residual block   not as graceful as results for VGG-16, since modern net-
        is well reduced by ourfeature map sampling.             works tend to have less redundancy by design.



                                                 1395                     Solution       Increased err.            [4] H. Bagherinezhad, M. Rastegari, and A. Farhadi. Lcnn:
                  Filter pruning [31]                            Lookup-based convolutional neural network.arXiv preprint 1.3(ﬁne-tuned, our impl.)                           arXiv:1611.06473, 2016.2
                    From scratch         1.9                [5] L. Breiman. Better subset regression using the nonnegative
                       Ours            2.0                   garrote.Technometrics, 37(4):373–384, 1995.3
                  Ours (ﬁne-tuned)       1.0                [6] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran,
         Table 8.2×speed-up comparisons for ResNet-56 on CIFAR-10,       B. Catanzaro, and E. Shelhamer. cudnn: Efﬁcient primitives
         the baseline accuracy is 92.8% (one view). We outperforms previ-       for deep learning.CoRR, abs/1410.0759, 2014.6
         ous approaches and scratch trained counterpart (smaller is better).    [7] F. Chollet. Xception: Deep learning with depthwise separa-
                                                            ble convolutions.arXiv preprint arXiv:1610.02357, 2016. 1,
                                                            2,3,4,6,7
         4.2.3 Experiments on CIFAR-10                      [8] M. Courbariaux and Y. Bengio. Binarynet: Training deep
                                                            neural networks with weights and activations constrained to+
         Even though our approach is designed for large datasets, it       1 or-1.arXiv preprint arXiv:1602.02830, 2016.1,2
         could generalize well on small datasets. We perform ex-    [9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
         periments on CIFAR-10 dataset [25], which is favored by       Fei. Imagenet: A large-scale hierarchical image database.
         many acceleration researches. It consists of 50k images for       InComputer Vision and Pattern Recognition, 2009. CVPR
         training and 10k for testing in 10 classes.                     2009. IEEE Conference on, pages 248–255. IEEE, 2009. 4
           We reproduce ResNet-56, which has accuracy of 92.8%    [10] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fer-
         (Serve as a reference, the ofﬁcial ResNet-56 [18] has ac-       gus. Exploiting linear structure within convolutional net-
         curacy of 93.0%). For2×acceleration, we follow similar       works for efﬁcient evaluation. InAdvances in Neural In-
                                                            formation Processing Systems, pages 1269–1277, 2014.2 setting as Sec.4.2.1(keep the ﬁnal stage unchanged, where
         the spatial size is8×8). Shown in Table8, our approach    [11] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,
                                                            and A. Zisserman. The PASCAL Visual Object Classes is competitive with scratch trained one, without ﬁne-tuning,       Challenge 2007 (VOC2007) Results. http://www.pascal- under2×speed-up. After ﬁne-tuning, our result is signif-       network.org/challenges/VOC/voc2007/workshop/index.html. icantly better than Filter pruning [31] and scratch trained       4,6
         one.                                            [12] R. Girshick. Fast r-cnn. InProceedings of the IEEE Inter-
                                                            national Conference on Computer Vision, pages 1440–1448,
         5. Conclusion                                      2015.2
                                                         [13] Y. Gong, L. Liu, M. Yang, and L. Bourdev. Compress-
           To conclude, current deep CNNs are accurate with high       ing deep convolutional networks using vector quantization.
         inference costs. In this paper, we have presented an       arXiv preprint arXiv:1412.6115, 2014.2
         inference-time channel pruning method for very deep net-    [14] Y. Guo, A. Yao, and Y. Chen. Dynamic network surgery for
         works. The reduced CNNs are inference efﬁcient networks       efﬁcient dnns. InAdvances In Neural Information Process-
         while maintaining accuracy, and only require off-the-shelf       ing Systems, pages 1379–1387, 2016.2
         libraries. Compelling speed-ups and accuracy are demon-    [15] S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz,
         strated for both VGG Net and ResNet-like networks on Im-       and W. J. Dally. Eie: efﬁcient inference engine on com-
         ageNet, CIFAR-10 and PASCAL VOC.                      pressed deep neural network. InProceedings of the 43rd
                                                            International Symposium on Computer Architecture, pages In the future, we plan to involve our approaches into       243–254. IEEE Press, 2016. 2 training time, instead of inference time only, which may    [16] S. Han, H. Mao, and W. J. Dally. Deep compression: Com- also accelerate training procedure.                          pressing deep neural network with pruning, trained quantiza-
                                                            tion and huffman coding.CoRR, abs/1510.00149, 2, 2015.
         References                                         2
                                                         [17] S. Han, J. Pool, J. Tran, and W. Dally. Learning both weights
          [1] J. M. Alvarez and M. Salzmann. Learning the number of       and connections for efﬁcient neural network. InAdvances in
            neurons in deep networks. InAdvances in Neural Informa-       Neural Information Processing Systems, pages 1135–1143,
            tion Processing Systems, pages 2262–2270, 2016. 1,2,3,       2015.1,2,3
            6                                           [18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
          [2] S. Anwar, K. Hwang, and W. Sung. Structured prun-       ing for image recognition.arXiv preprint arXiv:1512.03385,
            ing of deep convolutional neural networks. arXiv preprint       2015. 1,2,3,4,6,8
            arXiv:1512.08571, 2015.2                          [19] H. Hu, R. Peng, Y.-W. Tai, and C.-K. Tang. Network trim-
          [3] S. Anwar and W. Sung. Compact deep convolutional       ming: A data-driven neuron pruning approach towards efﬁ-
            neural networks with coarse pruning.  arXiv preprint       cient deep architectures. arXiv preprint arXiv:1607.03250,
            arXiv:1610.09639, 2016.1,2                            2016.2




                                                       1396         [20] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara,    [38] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
            A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, et al.       B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
            Speed/accuracy trade-offs for modern convolutional object       V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,
            detectors.arXiv preprint arXiv:1611.10012, 2016. 6            M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Ma-
         [21] S. Ioffe and C. Szegedy. Batch normalization: Accelerating       chine learning in Python.Journal of Machine Learning Re-
            deep network training by reducing internal covariate shift.       search, 12:2825–2830, 2011.4
            arXiv preprint arXiv:1502.03167, 2015.4                [39] A. Polyak and L. Wolf. Channel-level acceleration of deep
         [22] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up       face representations.IEEE Access, 3:2163–2175, 2015.2
            convolutional neural networks with low rank expansions.    [40] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-
            arXiv preprint arXiv:1405.3866, 2014.1,2,5,6,7              net: Imagenet classiﬁcation using binary convolutional neu-
         [23] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-       ral networks. InEuropean Conference on Computer Vision,
            shick, S. Guadarrama, and T. Darrell. Caffe: Convolu-       pages 525–542. Springer, 2016. 2
            tional architecture for fast feature embedding.arXiv preprint    [41] J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi. arXiv:1408.5093, 2014. 4,6                            You only look once: Uniﬁed, real-time object detection.
         [24] Y.-D. Kim, E. Park, S. Yoo, T. Choi, L. Yang, and D. Shin.       CoRR, abs/1506.02640, 2015. 6
            Compression of deep convolutional neural networks for    [42] S. Ren, K. He, R. B. Girshick, and J. Sun. Faster R-CNN:fast and low power mobile applications. arXiv preprint       towards real-time object detection with region proposal net-arXiv:1511.06530, 2015.2                             works.CoRR, abs/1506.01497, 2015.6 [25] A. Krizhevsky and G. Hinton. Learning multiple layers of    [43] K. Simonyan and A. Zisserman. Very deep convolutionalfeatures from tiny images. 2009.4,8                       networks for large-scale image recognition. arXiv preprint[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet       arXiv:1409.1556, 2014.3,4,5,6classiﬁcation with deep convolutional neural networks. In    [44] S. Srinivas and R. V. Babu. Data-free parameter pruningAdvances in neural information processing systems, pages       for deep neural networks.arXiv preprint arXiv:1507.06149,1097–1105, 2012.2,3                                2015.2[27] A. Lavin. Fast algorithms for convolutional neural networks.    [45] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,arXiv preprint arXiv:1509.09308, 2015.2                   D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.[28] V. Lebedev, Y. Ganin, M. Rakhuba, I. Oseledets, and       Going deeper with convolutions. InProceedings of the IEEEV. Lempitsky. Speeding-up convolutional neural net-       Conference on Computer Vision and Pattern Recognition,works using ﬁne-tuned cp-decomposition. arXiv preprint       pages 1–9, 2015.1,3,6arXiv:1412.6553, 2014.2                          [46] R. Tibshirani. Regression shrinkage and selection via the[29] V. Lebedev and V. Lempitsky. Fast convnets using group-       lasso. Journal of the Royal Statistical Society. Series Bwise brain damage.arXiv preprint arXiv:1506.02515, 2015.       (Methodological), pages 267–288, 1996.32                                           [47] N. Vasilache, J. Johnson, M. Mathieu, S. Chintala, S. Pi-[30] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-       antino, and Y. LeCun. Fast convolutional nets withbased learning applied to document recognition. Proceed-       fbfft: A gpu performance evaluation.  arXiv preprintings of the IEEE, 86(11):2278–2324, 1998.2,3                arXiv:1412.7580, 2014.1,2[31] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P.
            Graf. Pruning ﬁlters for efﬁcient convnets. arXiv preprint    [48] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learning
            arXiv:1608.08710, 2016.1,2,4,5,6,7,8                   structured sparsity in deep neural networks. InAdvances In
         [32] B. Liu, M. Wang, H. Foroosh, M. Tappen, and M. Pensky.       Neural Information Processing Systems, pages 2074–2082,
            Sparse convolutional neural networks. InProceedings of the       2016.1,2,3
            IEEE Conference on Computer Vision and Pattern Recogni-    [49] S. Xie, R. Girshick, P. Dollar, Z. Tu, and K. He. Aggregated´
            tion, pages 806–814, 2015.2                            residual transformations for deep neural networks. arXiv
         [33] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. E. Reed,       preprint arXiv:1611.05431, 2016.7
            C. Fu, and A. C. Berg. SSD: single shot multibox detector.    [50] J. Xue, J. Li, and Y. Gong. Restructuring of deep neural
            CoRR, abs/1512.02325, 2015.6                          network acoustic models with singular value decomposition.
         [34] Z. Mariet and S. Sra. Diversity networks. arXiv preprint       InINTERSPEECH, pages 2365–2369, 2013.2
            arXiv:1511.05077, 2015.2                          [51] T.-J. Yang, Y.-H. Chen, and V. Sze. Designing energy-
         [35] M. Mathieu, M. Henaff, and Y. LeCun. Fast training       efﬁcient convolutional neural networks using energy-aware
            of convolutional networks through ffts.  arXiv preprint       pruning.arXiv preprint arXiv:1611.05128, 2016.2
            arXiv:1312.5851, 2013.2                          [52] X. Zhang, J. Zou, K. He, and J. Sun. Accelerating very
         [36] V. Nair and G. E. Hinton. Rectiﬁed linear units improve       deep convolutional networks for classiﬁcation and detection.
            restricted boltzmann machines. InProceedings of the 27th       IEEE transactions on pattern analysis and machine intelli-
            international conference on machine learning (ICML-10),       gence, 38(10):1943–1955, 2016.1,2,3,5,6,7
            pages 807–814, 2010.4
         [37] J. Nickolls, I. Buck, M. Garland, and K. Skadron. Scalable
            parallel programming with CUDA.ACM Queue, 6(2):40–53,
            2008.6




                                                       1397