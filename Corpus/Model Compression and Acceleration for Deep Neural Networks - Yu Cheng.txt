         Deep learning for visual unDerstanDing:
         part 2


                                                                     Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang







        Model Compression and Acceleration  


        for Deep Neural Networks


        The principles, progress, and challenges












                                                            In recent years, deep neural networks (DNNs) have received 
                                                              increased attention, have been applied to different applica-
                                                              tions, and achieved dramatic accuracy improvements in many 
                                                            tasks. These works rely on deep networks with millions or even 
                                                            billions of parameters, and the availability of graphics process-
                                                            ing units (GPUs) with very high computation capability plays 
                                                            a key role in their success. For example, Krizhevsky et al. [1] 
                                                            achieved breakthrough results in the 2012 ImageNet Challenge 
                                                            using a network containing 60 million parameters with five 
                                                            convolutional layers and three fully connected layers. Usu-
                                                            ally, it takes two to three days to train the whole model on the 
                                                            ImagetNet data set with an NVIDIA K40 machine. In another 
                                                            example, the top face-verification results from the Labeled 
                                                            Faces in the Wild (LFW) data set were obtained with networks 
                                                            containing hundreds of millions of parameters, using a mix 
                                                            of convolutional, locally connected, and fully connected layers 
                                                            [2], [3]. It is also very time-consuming to train such a model 
                                                            to obtain a reasonable performance. In architectures that only 
                                                            rely on fully connected layers, the number of parameters can 
                                                            grow to billions [4].


                                                            Introduction
                                                            As larger neural networks with more layers and nodes are 
                                                            considered, reducing their storage and computational cost 
                                                            becomes critical, especially for some real-time applications  ©Istockphoto.com/zapp2photo
                                                            such as online learning and incremental learning. In addition, 
                                                            recent years witnessed significant progress in virtual real-
                                                            ity, augmented reality, and smart wearable devices, creating 
                                                            unprecedented opportunities for researchers to tackle fun-
                                                            damental challenges in deploying deep-learning systems to 
                                                            portable devices with limited resources [e.g., memory, central 
                                                            processing units (CPUs), energy, bandwidth]. Efficient deep-
                                                            learning methods can have a significant impact on distributed 
                                                            systems, embedded devices, and field-programmable gate ar-
                                                            ray (FPGA) for artificial intelligence (AI). For example, the 
                                                            residual network-50 (ResNet-50) [5], which has 50 convolu-
                                                            tional layers, needs more than 95 megabytes of memory for  Digital Object Identifier 10.1109/MSP.2017.2765695
        Date of publication: 9 January 2018                                  storage, and numerous floating number multiplications for 


      126                                   IEEE SIgnal ProcESSIng MagazInE    |   January 2018    |                     1053-5888/18©2018IEEE       calculating each image. After discarding    As larger neural networks   volutional layers only. Low-rank factoriza-
       some redundant weights, the network still    with more layers and       tion and transferred/compact filters-based 
       works as usual but saved more than 75% of    nodes are considered,      approaches provide an end-to-end pipeline 
       parameters and 50% computational time.    reducing their storage      and can be easily implemented in a CPU/
       For devices like cell phones and FPGAs                            GPU environment, which is straightfor-
       with only several megabyte resources, how                           and computational         ward, while parameter pruning and sharing 
       to compact the models used on them is     cost becomes critical,      use different methods such as vector quan-
       also important.                      especially for some real-   tization, binary coding, and sparse con-
         Achieving these goals calls for joint    time applications such      straints to perform the task. Usually, it will 
       solutions from many disciplines, including    as online learning and      take several steps to achieve the goal.
       but not limited to machine learning, opti-    incremental learning.        Regarding training protocols, models 
       mization, computer architecture, data com-                           based on parameter pruning/sharing low-
       pression, indexing, and hardware design.                            rank factorization can be extracted from 
       In this article, we review recent works on compressing and   pretrained ones or trained from scratch, while the transferred/
       accelerating DNNs, which attracted much attention from the   compact filter and KD models can only support training from 
       deep-learning community and has already achieved signifi-   scratch. These methods are independently designed and com-
       cant progress in past years.                          plement each other. For example, transferred layers and pa-
         We classify these approaches into four categories:         rameter pruning and sharing can be used together, and model 
       1) Parameter pruning and sharing: The parameter pruning   quantization and binarization can be used together with low-
         and sharing-based methods explore the redundancy in the   rank approximations to achieve further speedup. We will de-
         model parameters and try to remove the redundant and   scribe the details of each theme and their properties, strengths, 
         noncritical ones.                               and drawbacks in the following sections.
       2) Low-rank factorization: Low-rank factorization-based 
         techniques use matrix/tensor decomposition to estimate the   Parameter pruning and sharing
         informative parameters of the deep convolutional neural   An early work that showed that network pruning is effective in 
         networks (CNNs).                               reducing the network complexity and addressed the overfitting 
       3) Transferred/compact convolutional filters: The trans-   problem is [6]. Since then, it has been widely studied to compress 
         ferred/compact convolutional filters-based approaches   DNN models, trying to remove parameters that are not crucial to 
         design special structural convolutional filters to reduce the   the model performance. These techniques can be further classi-
         storage and computation complexity.                 fied into three categories: model quantization and binarization, 
       4) Knowledge distillation (KD): The KD methods learn a dis-   parameter sharing, and structural matrix.
         tilled model and train a more compact neural network to 
         reproduce the output of a larger network.               Quantization and binarization
         In Table 1, we briefly summarize these four types of meth-   Network quantization compresses the original network by 
       ods. Generally, the parameter pruning and sharing, low-rank   reducing the number of bits required to represent each weight. 
       factorization, and KD approaches can be used in DNNs with   Gong et al. [6] and Wu et al. [7] applied k-means scalar quanti-
       fully connected layers and convolutional layers, achieving   zation to the parameter values. Vanhoucke et al. [8] showed that 
       comparable performances. On the other hand, methods using   8-bit quantization of the parameters can result in significant 
       transferred/compact filters are designed for models with con-   speedup with minimal loss of accuracy. The work in [9] used 




        Table 1. A summary of different approaches for network compression.
        Theme Name            Description                   Applications          More Details 
        Parameter pruning and sharing  Reducing redundant parameters that      Convolutional layer and     Robust to various settings, can achieve 
                          are not sensitive to the performance     fully connected layer     good performance, can support both train-
                                                                    ing from scratch and pretrained model
        Low-rank factorization       Using matrix/tensor decomposition to     Convolutional layer and     Standardized pipeline, easily implement-
                          estimate the informative parameters      fully connected layer     ed, can support both training from scratch 
                                                                    and pretrained model
        Transferred/compact        Designing special structural convolutional   Only for convolutional layer Algorithms are dependent on applications, 
        convolutional filters        filters to save parameters                           usually achieve good performance, only 
                                                                    support training from scratch
        KD                 Training a compact neural network with    Convolutional layer and     Model performances are sensitive to  
                          distilled knowledge of a large model    fully connected layer     applications and network structure, only 
                                                                    support training from scratch


                                     IEEE SIgnal ProcESSIng MagazInE    |   January 2018    |                             127       16-bit fixed-point representation in stochastic rounding-based   er drawback of these binary nets is that existing binarization 
       CNN training, which significantly reduced memory usage and   schemes are based on simple matrix approximations and ignore 
       float- point operations with little loss in classification accuracy.  the effect of binarization on the accuracy loss. To address 
         The method proposed in [10] first pruned the unimportant con-  this issue, the work in [17] proposed a proximal Newton algo-
       nections and retrained the sparsely connected networks. Then it   rithm with diagonal Hessian approximation that directly mini-
       quantized the link weights using weight-sharing, and then applied   mizes the loss with respect to the binary weights. The work in 
       Huffman coding to the quantized weights as                           [18] significantly reduced the time on float-
       well as the codebook to further reduce the                           point multiplication in the training stage by 
       rate. As shown in Figure 1     , it starts by learn-    Network pruning and       stochastically binarizing weights and con-
       ing the connectivity via normal network train-   sharing has been used      verting multiplications in the hidden state 
       ing, followed by pruning the small-weight    both to reduce network     computation to sign changes.
       connections. Finally, the network is retrained    complexity and to address  to learn the final weights for the remaining    the overfitting issue.       Pruning and sharing
       sparse connections. This work achieves the                            Network pruning and sharing has been used 
       state-of-the-art performance among all param-                           both to reduce network complexity and to 
       eter quantization-based methods. It was shown in [11] that Hes-  address the overfitting issue. An early approach to pruning was 
       sian weight could be used to measure the importance of network   biased weight decay [19]. The optimal brain damage [20] and 
       parameters and proposed to minimize Hessian-weighted quantiza-  the optimal brain surgeon [21] methods reduced the number 
       tion errors on average for clustering network parameters. A novel   of connections based on the Hessian of the loss function, and 
       quantization framework was introduced in [12], which reduced the   their works suggested that such pruning gave higher accuracy 
       precision of network weights to ternary values.              than magnitude-based pruning such as the weight decay meth-
         In the extreme case of 1-bit representation of each weight, i.e.,   od. Those methods supported training from scratch. 
       binary weight neural networks, there are also many works that     A recent trend in this direction is to prune redundant, non-
       directly train CNNs with binary weights; for instance, Binary-   informative weights in a pretrained CNN model. For example, 
       Connect [13], BinaryNet [14], and XNORNetworks [15]. The   Srinivas and Babu [22] explored the redundancy among neurons 
       main idea is to directly learn binary weights or activations dur-   and proposed a data-free pruning method to remove redundant 
       ing the model training. The systematic study in [16] showed that   neurons. Han et al. [23] proposed to reduce the total number of 
       networks trained with backpropagation could be robust against   parameters and operations in the entire network. Chen et al. [24] 
       (robust against or resilient to) specific weight distortions, includ-   proposed a HashedNets model that used a low-cost hash function 
       ing binary weights.                               to group weights into hash buckets for parameter sharing. The 
                                                   deep compression method in [10] removed the redundant connec-
       Drawbacks                                    tions and quantized the weights and then used Huffman coding 
       However, the accuracy of such binary nets is significantly low-   to encode the quantized weights. In [25], a simple regularization 
       ered when dealing with large CNNs such as GoogleNet. Anoth-   method based on soft weight-sharing was proposed, which 







                                             Cluster the Weights

                   Train ConnectivityOriginal                                                                    Compressed
        Network                                                                     NetworkGenerate Codebook           Encode Weights

                  Prune Connections
                                            Quantize the Weights
                                              with Codebook              Encode Index
                    Train Weights

                                             Retrain Codebook




       Figure 1.  The three-stage compression method proposed in [10]: pruning, quantization, and encoding. The input is the original model, and the output is 
       the compression model.


     128                             IEEE SIgnal ProcESSIng MagazInE    |   January 2018    |       included both quantization and pruning in one simple (re)train-   Thus the memory cost becomes O()d instead of O()d2 . 
       ing procedure. It is worth noting that the aforementioned prun-   This circulant structure also enables the use of fast Fou-
       ing schemes typically produce connection pruning in CNNs.    rier transform (FFT) to speed up the computation. Given a 
         There is also growing interest in training compact CNNs   d-dimensional vector r, the 1-layer circulant neural network 
       with sparsity constraints. Those sparsity constraints are    in (1) has time complexity of O()ddlog .
       typically introduced in the optimization                               In [31], a novel adaptive fastfood trans-
       problem as l0  or l1 -norm regularizers.    CNNs are parameter-efficient  form was introduced to reparameterize the 
       The work in [26] imposed group sparsity    due to exploring the         matrix-vector multiplication of fully con-
       constraints on the convolutional filters to                            nected layers. The adaptive fastfood trans-
       achieve structured brain damage, i.e., prun-    translation invariant property  form matrix RR! nd#  was defined as
       ing entries of the convolution kernels in a    of the representations to 
       group-wise fashion. In [27], a group-sparse    input image, which is the key          RS= HGPHB. (2)
       regularizer on neurons was introduced    to the success of training  during the training stage to learn compact    very deep models without    Here, SG,, and B are random diago-
       CNNs with reduced filters. Wen et al. [28]                            nal matrices. P!{,01}dd#
                                       severe overfitting.                              is a random 
       added a structured sparsity regularizer on                            permutation matrix and H denotes the 
       each layer to reduce trivial filters, chan-                            Walsh–Hadamard matrix. Reparameteriz-
       nels, or even layers. In filter-level pruning, all of the afore-   ing a fully connected layer with  d inputs and n outputs using 
       mentioned works used l21, -norm regularizers. The work in [29]   the adaptive fastfood transform reduces the storage and the 
       used l1 -norm to select and prune unimportant filters.         computational costs from O()nd to O()n and from O()nd to 
                                                   O()ndlog , respectively.
       Drawbacks                                      The work in [32] showed the effectiveness of the new notion 
       There are some potential issues of the pruning and sharing   of parsimony in the theory of structured matrices. Their pro-
       works. First, pruning with l1  or l2  regularization requires   posed method can be extended to various other structured matrix 
       more iterations to converge. Furthermore, all pruning criteria   classes, including block and multilevel Toeplitz-like [33] matrices 
       require manual setup of sensitivity for layers, which demands   related to multidimensional convolution [34].
       fine-tuning of the parameters and could be cumbersome for 
       some applications.                               Drawbacks
                                                   One potential problem of this kind of approach is that the struc-
       Designing the structural matrix                        tural constraint will cause loss in accuracy since the constraint 
       In architectures that contain only fully connected layers, the   might bring bias to the model. On the other hand, how to find a 
       number of parameters can grow up to billions [4]. Thus, it is   proper structural matrix is difficult. There is no theoretical way 
       critical to explore this redundancy of parameters in fully con-   from which to derive it.
       nected layers, which is often the bottleneck in terms of memory 
       consumption. These network layers use the nonlinear transforms   Low-rank factorization and sparsity
       f(,xM)(=v Mx), where  v ()o is an element-wise nonlinear   As convolution operations constitute the bulk of all computations 
       operator, x is the input vector, and M is the mn#  matrix of   in CNNs, simplifying the convolution layer would have a direct 
       parameters. When M is a large general dense matrix, the cost   impact on the overall speedup. The convolution kernels in a typi-
       of storing mn parameters and computing matrix-vector products   cal CNN is a four-dimensional tensor. The key observation is that 
       in Om()n time. Thus, an intuitive way to prune parameters is to   there might be a significant amount of redundancy in the tensor. 
       impose x as a parameterized structural matrix. An mn#  matrix   Ideas based on tensor decomposition seem to be a particularly 
       that can be described using much fewer parameters than mn is   promising way to remove the redundancy. Regarding to the fully 
       called a structured matrix. Typically, the structure should not   connected layer, it can be viewed as a two-dimensional (2-D) 
       only reduce the memory cost but also dramatically accelerate the   matrix and the low-rankness can also help.
       inference and training stage via fast matrix-vector multiplication     Using low-rank filters to accelerate convolution has a long 
       and gradient computations.                          history. Typical examples include high-dimensional discrete 
         Following this direction, the work in [30] proposed a sim-   cosine transform (DCT) and wavelet systems constructed 
       ple and efficient approach based on circulant projections,   from one-dimensional (1-D) DCT transform and 1-D wave-
       while maintaining competitive error rates. Given a vector   lets, respectively, using tensor products. In the context of 
       r=(,rr 01 ,,frd-1 ),  a circulant matrix RR! dd#  is defined as   dictionary learning, Rigamonti et al. [35] suggested learning 
                                                   separable 1-D filters. In [36], a few low-rank approximation Rr0 rd 1 g r    VS     -      2  r1 W          and clustering schemes for the convolutional kernels were 
                       Sr1  r0 rd 1    r W          proposed. They achieved 2# speedup for a single convolu-
            Rr (circ ): S        -     2
             ==r           WS h   1  r0 j  h W. (1)   tional layer with 1% drop in classification accuracy. The 
                       Srd-2     j jrd-1 W          work in [37] suggested using different tensor decomposition Sr               WTd-1 rd-2 g r1  r0 X          schemes, reporting a 45.# speedup with 1% drop in accuracy 


                                     IEEE SIgnal ProcESSIng MagazInE    |   January 2018    |                             129                                                                  case. For the scheme in [39], the decom-
                                                                  position always exists and can achieve 
                                                                  better performance than general CP. 
                                                                  Table 2 lists a performance comparison 
                                                                  of both methods. The actual speedup 
                                                                  and compression rates are used to mea-
                                                                  sure the performances. We can see that 
                                                                  the BN version can achieve slightly bet-
                                                                  ter performance while the CP version 
                                                                  gives higher compression rates. Original Framework             Low-Rank                          Note that the fully connected layers  Factorization Framework               can be viewed as a 2-D matrix and thus 
                (a)                            (b)                 the aforementioned methods can also 
                                                                  be applied there. There are several clas-
                                                                  sical works on exploiting low-rankness  Figure 2.  A typical framework of the low-rank regularization method. (a) is theoriginal convolutional 
       layer, and (b) is the low-rank constraint convolutional layer with rank-K.                   in fully connected layers. For instance, 
                                                                  Misha et al. [40] reduced the number 
                                                                  of dynamic parameters in deep models 
       in text recognition. In both works, the approximation was   using the low-rank method. Reference [41] explored a low-rank 
       done layer by layer. After one layer was approximated by   matrix factorization of the final weight layer in a DNN for 
       the low-rank filters, the parameters of that layer were fixed,   acoustic modeling.
       and the layers above were fine-tuned based on a reconstruc-
       tion error criterion. These are typical low-rank methods for   Drawbacks
       compressing 2-D convolutional layers, which is described in   Low-rank approaches are straightforward for model compres-
       Figure 2. In [38], canonical polyadic (CP) decomposition of   sion and acceleration. The idea complements recent advances 
       the kernel tensors was proposed. Their work used nonlinear   in deep learning such as dropout, rectified units, and maxout. 
       least squares to compute the CP decomposition, which was   However, the implementation is not that easy since it involves 
       also based on the tensor decomposition idea. In [39], a new   a decomposition operation, which is computationally expen-
       algorithm for computing the low-rank tensor decomposition   sive. Another issue is that current methods perform low-rank 
       and a new method for training low-rank constrained CNNs   approximation layer by layer, and thus cannot perform global 
       from scratch were proposed. It used batch normalization (BN)   parameter compression, which is important as different lay-
       to transform the activations of the internal hidden units, and it   ers hold different information. Finally, factorization requires 
       was shown to be an effective way to deal with the exploding   extensive model retraining to achieve convergence when com-
       or vanishing gradients.                            pared to the original model.
         In principle, both the CP decomposition scheme and the 
       decomposition scheme in [39] (BN low-rank) can be used to   Transferred/compact convolutional filters
       train CNNs from scratch. For the CP decomposition, finding   CNNs are parameter-efficient due to exploring the transla-
       the best low-rank approximation is an ill-posed problem, and   tion invariant property of the representations to input image, 
       the best rank-K approximation may not exist in the general   which is the key to the success of training very deep models 
                                                   without severe overfitting. Although a strong theory is cur-
                                                   rently missing, a large amount of empirical evidence sup-
                                                   ports the notion that both the translation invariant property  Table 2. Comparisons between the low-rank models and their baselines 
        on ILSVRC-2012.                                 and convolutional weight-sharing are important for good 
                                                   predictive performance. The idea of using transferred con-Model       TOP-5 Accuracy  Speedup   Compression Rate   volutional filters to compress CNN models is motivated by 
        AlexNet      80.03%       1        1             recent works in [42], which introduced the equivariant group 
        BN low-rank   80.56%       1.09      4.94           theory. Let x be an input, U()$ be a network or layer, and 
                                                   T()$ be the transform matrix. The concept of equivariance  CP low-rank   79.66%       1.82      5             is defined as VGG-16      90.60%       1        1 
        BN low-rank   90.47%       1.53      2.72                        TTlUU ^^ xx hh =     , (3)
        CP low-rank   90.31%       2.05      2.75 
        GoogleNet    92.21%       1        1             which says that transforming the input x by the transform 
                                                   T()$ and then passing it through the network or layer U(·) BN low-rank   91.88%       1.08      2.79           should give the same result as first mapping x through the  CP low-rank   91.79%       1.20      2.84           network and then transforming the representation. Note that, 


     130                             IEEE SIgnal ProcESSIng MagazInE    |   January 2018    |       in [42], the transforms T()$ and Tl()$ are not necessarily   where Tx(·,,y) denoted the translation of the first oper-
       the same as they operate on different objects. According to   and by (,xy) along its spatial dimensions, with proper zero 
       this theory, it is reasonable to apply the transform to layers   padding at borders to maintain the shape. The proposed 
       or filters U()$ to compress the whole network models. From   framework can be used to 1) improve the classification accu-
       empirical observation, deep CNNs also benefit from using a   racy as a regularized version of maxout networks and 2) 
       large set of convolutional filters by applying a certain trans-   to achieve parameter efficiency by flexibly varying their 
       form T()$ to a small set of base filters since it acts as a regu-   architectures to compress networks.
       larizer for the model.                                Table 3 briefly compares the performance of different 
         Following this trend, there are many recent works proposed   methods with transferred convolutional filters, using VGG-
       to build a convolutional layer from a set of base filters [42]–   Net (16 layers) as the baseline model. The results are report-
       [45]. What they have in common is that the transform T()$   ed on the CIFAR-10 and CIFAR-100 data sets with top-five 
       lies in the family of functions that only operate in the spatial   error rates. It is observed that they can achieve reduction in 
       domain of the convolutional filters. For                            parameters with little or no drop in clas-
       example, the work in [44] found that the                            sification accuracy.
       lower convolution layers of CNNs learned    The basic idea of KD is to 
       redundant filters to extract both positive and    distill knowledge from a    Drawbacks
       negative phase information of an input sig-    large teacher model into     There are several issues that need to be 
       nal, and defined T()$  to be the simple nega-    a small one by learning      addressed for approaches that apply transfer 
       tion function                        the class distributions      information to convolutional filters. First, 
                                        output by the teacher       these methods can achieve competitive per-
               T^h WW x =  -x . (4)                           formance for wide/flat architectures (like  via softened softmax.      VGGNet) but not narrow/special ones (like 
       Here, Wx  is the basis convolutional filter                           GoogleNet and ResNet). Second, the trans-
       and W-x  is the filter consisting of the shifts whose activation is   fer assumptions sometimes are too strong to guide the algo-
       opposite to that of Wx  and selected after max-pooling opera-   rithm, making the results unstable on some data sets.
       tion. By doing this, the work in [44] can easily achieve 2# com-     Using a compact filter for convolution can directly reduce 
       pression rate on all the convolutional layers. It is also shown that   the computation cost. The key idea is to replace the loose and 
       the negation transform acts as a strong regularizer to improve   overparametric filters with compact blocks to improve the 
       the classification accuracy. The intuition is that the learning   speed, which significantly accelerate CNNs on several bench-
       algorithm with pair-wise positive-negative constraint can lead   marks. Decomposing 33#  convolution into two 11#  con-
       to useful convolutional filters instead of redundant ones.       volutions was used in [47], which achieved state-of-the-art 
         In [45], it was observed that magnitudes of the responses   acceleration performance on object recognition. SqueezeNet 
       from convolutional kernels had a wide diversity of pattern rep-   [48] was proposed to replace 33#  convolution with 11#  
       resentations in the network, and it was not proper to discard   convolution, which created a compact neural network with 
       weaker signals with a single threshold. Thus, a multibias non-   approximately 50 fewer parameters and comparable accuracy 
       linearity activation function was proposed to generate more   when compared to AlexNet.
       patterns in the feature space at low computational cost. The 
       transform T()$ was define as                        KD
                                                   To the best of our knowledge, exploiting knowledge transfer to 
                    TlU^h xW=+ x d , (5)   compress model was first proposed by Caruana et al. [49]. They 
                                                   trained a compressed model with pseudo-data labeled by an 
       where  d  were the multibias factors. The work in [46] consid-   ensemble of strong classifiers and reproduced the output of the 
       ered a combination of rotation by a multiple of 90° and hori-   original larger network. However, their work is limited to shal-
       zontal/vertical flipping with                         low models. The idea has been recently adopted in [50] as KD 
                                                   to compress deep and wide networks into shallower ones, where 
                     TlU^h xW=  Ti , (6)
                                                    Table 3. Comparisons of different approaches based on transferred  where WTi  was the transformation matrix that rotated the orig-   convolutional filters on CIFAR-10 and CIFAR-100.
       inal filters with angle  i !{90,,}180270. In [42], the transform    Model        CIFAR-100   CIFAR-10   Compression Rate was generalized to any angle learned from data, and  i  was 
       directly obtained from data. Both [46] and [42] can achieve    VGG-16       34.26%     9.85%     1 
       good classification performance.                       MBA [45]      33.66%     9.76%     2 
         Reference [43] defined T()$ as the set of translation func-   CRELU [44]     34.57%     9.92%     2 
       tions applied to 2-D filters                           CIRC [42]      35.15%     10.23%    4 
             T lU^^ xhh =Tx·,,y              ,  (7)   DCNN [43]     33.57%     9.65%     1.62  xy,,!" -kkf,, ,^ xy,( h !00,)


                                     IEEE SIgnal ProcESSIng MagazInE    |   January 2018    |                             131       the compressed model mimicked the function learned by the   Other types of approaches
       complex model. The basic idea of KD is to distill knowledge   We first summarize the works utilizing attention-based 
       from a large teacher model into a small one by learning the   methods. Note that attention-based systems [57] can reduce 
       class distributions output by the teacher via softened softmax.   computations significantly by learning to selectively focus or 
         The work in [51] introduced a KD compression framework,   “attend to” a few, task-relevant input regions. The work in [57] 
       which eased the training of deep networks by following a student-  introduced the dynamic capacity network that combined two 
       teacher paradigm, in which the student was penalized according   types of modules: the small subnetworks with low capacity, and 
       to a softened version of the teacher’s output. The framework   the large ones with high capacity. The low-capacity subnetworks 
       compressed an ensemble of deep networks (teacher) into a stu-  were active on the whole input to first find the task-relevant areas 
       dent network of similar depth. To do so, the student was trained   in the input, and then the attention mechanism was used to di-
       to predict the output of the teacher, as well as the true classifica-  rect the high-capacity subnetworks to focus on the task-relevant 
       tion labels. Despite its simplicity, KD demonstrates promising   regions in the input. By doing this, the size of the CNN model 
       results in various image classification tasks. The work in [52]   could be significantly reduced.
       aimed to address the network compression                              Following this direction, the work in 
       problem by taking advantage of depth neural    The standard criteria       [58] introduced the conditional computation 
       networks. It proposed an approach to train    to measure the quality      idea, which only computes the gradient for 
       thin and deep networks, called FitNets, to    of model compression      some important neurons. It proposed a new 
       compress wide and shallower (but still deep)    and acceleration are the    type of general-purpose neural network com-
       networks. The method was rooted in KD and                           ponent: a sparsely gated mixture-of-experts 
       extended the idea to allow for thinner and    compression and the       (MoE) layer. The MoE consisted of a number 
       deeper student models. To learn from the    speedup rates.            of experts, each a simple feed-forward neural 
       intermediate representations of the teacher                           network, and a trainable gating network that 
       network, FitNet made the student mimic the full feature maps of   selected a sparse combination of the experts to process each input. 
       the teacher. However, such assumptions are too strict since the   In [59], dynamic DNNs (D2NNs) were introduced, which were a 
       capacities of teacher and student may differ greatly. In certain   type of feed-forward DNN that selected and executed a subset of 
       circumstances, FitNet may adversely affect the performance and   D2NN neurons based on the input.
       convergence. All the aforementioned methods are validated on     There have been other attempts to reduce the number of 
       the MNIST, CIFAR-10, CIFAR-100, SVHN, and AFLW bench-  parameters of neural networks by replacing the fully con-
       mark data sets, and simulation results show that these methods   nected layer with global average pooling [43], [60]. Network 
       match or outperform the teacher’s performance, while requiring   architectures, such as GoogleNet or network in network, 
       notably fewer parameters and multiplications.              can achieve state-of-the-art results on several benchmarks 
         There are several extensions along this direction of distilla-   by adopting this idea. However, transfer learning, i.e., reus-
       tion knowledge. The work in [53] trained a parametric student   ing features learned on the ImageNet data set and applying 
       model to approximate a Monte Carlo teacher. The proposed   them to new tasks, is more difficult with this approach. This 
       framework used online training and used DNNs for the student   problem was noted by Szegedy et al. [60] and motivated 
       model. Different from previous works, which represented the   them to add a linear layer on  top of their networks to enable 
       knowledge using the softened label probabilities, [54] repre-   transfer learning.
       sented the knowledge by using the neurons in the higher hidden     The work in [61] targeted the ResNet-based model with a 
       layer, which preserved as much information as the label prob-   spatially varying computation time, called stochastic depth, 
       abilities, but are more compact. The work in [55] accelerated   which enabled the seemingly contradictory setup to train short 
       the experimentation process by instantaneously transferring   networks and used deep networks at test time. It started with 
       the knowledge from a previous network to each new deeper   very deep networks and, while during training, for each mini-
       or wider network. The techniques are based on the concept   batch, randomly dropped a subset of layers and bypassed them 
       of function-preserving transformations between neural net-   with the identity function. This model is end-to-end trainable, 
       work specifications. Zagoruyko et al. [56] proposed attention   deterministic, and can be viewed as a black-box feature extrac-
       transfer to relax the assumption of FitNet. They transferred the   tor. Following this direction, the work in [62] proposed a pyra-
       attention maps that are summaries of the full activations.      midal residual network with stochastic depth.
                                                     Other approaches to reduce the convolutional overheads 
       Drawbacks                                    include using FFT-based convolutions [63] and fast convolution 
       KD-based approaches can make deeper models thinner and   using the Winograd algorithm [64]. Those works only aim to 
       help significantly reduce the computational cost. However,   speedup the computation but not reduce the memory storage.
       there are a few disadvantages. One of them is that KD can only 
       be applied to classification tasks with softmax loss function,   Benchmarks, evaluation, and databases
       which hinders its usage. Another drawback is that the model   In the past five years, the deep-learning community has made 
       assumptions sometimes are too strict to make the performance   great efforts in benchmark models. One of the most well-
       competitive with other types of approaches.               known models used in compression and acceleration for CNNs 


     132                             IEEE SIgnal ProcESSIng MagazInE    |   January 2018    |       is Alexnet [1], which occasionally has been    Proposing some general/   about how to choose different compression 
       used for assessing the performance of com-    unified approaches is       approaches and possible challenges/solu-
       pression. Other popular standard models    one direction that can       tions in this area.
       include LeNets [65], All-CNN-nets [66],    be taken regarding  and many others. LeNet-300-100 is a fully                            General suggestions
       connected network with two hidden layers,    the use of CNNs in          There is no golden rule to measure which one 
       with 300 and 100 neurons each. LeNet-5 is    small platforms.          of the four kinds of approaches is the best. How 
       a convolutional network that has two convo-                           to choose the proper approaches is really de-
       lutional layers and two fully connected layers. Recently, more   pendent on the applications and requirements. Here, we provide 
       state-of-the-art architectures are used as baseline models in   some general suggestions.
       many works, including network in networks [67], VGGNets   ■  If the applications needs compacted models from pretrained 
       [68], and ResNets [69]. Table 4 summarizes the baseline mod-     models, one can choose either pruning and sharing or low-
       els commonly used in several typical compression methods.      rank factorization-based methods. If end-to-end solutions 
         The standard criteria to measure the quality of model com-     are needed for the problem, the low-rank and transferred 
       pression and acceleration are the compression and the speedup     convolutional filters approaches are preferred.
       rates. Assume that a is the number of the parameters in the   ■  For applications in some specific domains, methods with 
       original model M and a*  is that of the compressed model M* ,     human prior (like the transferred convolutional filters and 
       then the compression rate  a (,MM * ) of  M*  over M is          structural matrix) sometimes have benefits. For example, 
                                                     when conducting medical images classification, transferred 
                       MM,.aa ^h * =   (8)a                      convolutional filters should work well as medical images  *                     (like organs) do have the rotation transformation property.
       Another widely used measurement is the index space saving   ■  Usually, the approaches of pruning and sharing could give 
       defined in several papers [70], [71] as                     a reasonable compression rate while not hurting the accu-
                                                     racy. Thus, for applications that require stable model accu-
                      MM,,aa b    * = -^h *  (9)a                      racy, it is better to utilize pruning and sharing. *
                                                   ■  If a problem involves small- or medium-size data sets, one 
       where a and a are the number of the dimension of the index     can try the KD approaches. The compressed student model 
       space in the original model and that of the compressed      can take the benefit of transferring knowledge from the 
       model, respectively.                                teacher model, making it a robust data set that is not large.
         Similarly, given the running time s of M and s*  of M*,  the   ■  As we mentioned in the “Introduction,” techniques of the 
       speedup rate  d (,MM * ) is defined as                      four themes are orthogonal. It makes sense to combine two 
                                                     or three of them to maximize the compression/speedup 
                      MM,.sd ^h * =s  (10)     rates. For some specific applications, like object detection,  *                      which requires both convolutional and fully connected lay-
       Most work used the average training time per epoch to mea-     ers, one can compress the convolutional layers with low-
       sure the running time, while in [70] and [71], the average     rank factorization and the fully connected layers with a 
       testing time was used. Generally, the compression rate and     pruning method.
       speedup rate are highly correlated, as smaller models often 
       results in faster computation for both the training and the 
       testing stages.
         Good compression methods are expected to achieve almost    Table 4. A summary of baseline models used in  
       the same performance as the original model with much smaller    different representative works of network compression.
       parameters and less computational time. However, for differ-   Baseline Models        Representative Works 
       ent applications with varying CNN designs, the correlation    Alexnet [1]           Structural matrix [30]–[32]  between parameter size and computational time may be dif-
       ferent. For example, it is observed that, for deep CNNs with                     Low-rank factorization [39] 
       fully connected layers, most of the parameters are in the fully    Network in network [67]   Low-rank factorization [39] 
       connected layers; while for image classification tasks, float-   VGGNets [68]         Transferred filters [43] 
       point operations are mainly in the first few convolutional lay-                    Low-rank factorization [39]  ers since each filter is convolved with the whole image, which    ResNets [69]          Compact filters [48], stochastic depth [61] is usually very large at the beginning. Different applications 
       should focus on different layers.                                         Parameter sharing [25] 
                                                    All-CNN-nets [66]       Transferred filters [44] 
       Discussion and challenges                         LeNets [65]           Parameter sharing [25] 
       In this article, we summarized recent works on compress-                    Parameter pruning [21], [23]  ing and accelerating DNNs. Here we discuss more details 


                                     IEEE SIgnal ProcESSIng MagazInE    |   January 2018    |                             133      Technique challenges               good compression        approaches. Instead of directly reducing 
      Techniques for deep model compression    methods are expected     and transferring parameters from the teach-
      and acceleration are still in the early stages,    to achieve almost the      er models, passing selectivity knowledge of 
      and the following challenges still need to    same performance as the   neurons could be helpful. One can derive 
      be addressed.                                        a way to select essential neurons related to  original model with much  ■  Most of the current state-of-the-art ap  -                       the task. The intuition is that, if a neuron 
        proaches are built on well-designed    smaller parameters and    is activated in certain regions or samples, 
        CNN models, which have limited free-   less computational time.    this implies these regions or samples share 
        dom to change the configuration (e.g.,                        some common properties that may relate 
        network structural, hyperparameters).                        to the task. Performing such steps is time-
        To handle more complicated tasks, it should provide more   consuming, thus efficient implementation is important.
        plausible ways to configure the compressed models.        For methods with convolutional filters and the structural 
      ■  Pruning is an effective way to compress and accelerate   matrix, we can conclude that the transformation lies in the 
        CNNs. Current pruning techniques are mostly designed to   family of functions that only operations on the spatial dimen-
        eliminate connections between neurons. On the other hand,   sions. Hence, to address the imposed prior issue, one solution 
        a pruning channel can directly reduce the feature map   is to provide a generalization of the aforementioned approach-
        width and shrink the model into a thinner one. It is efficient   es in two aspects: 1) instead of limiting the transformation 
        but also challenging because removing channels might dra-  to belong to a set of predefined transformations, let it be the 
        matically change the input of the following layer. It is   whole family of spatial transformations applied to 2-D filters 
        important to focus on how to address this issue.         or the matrix, and 2) learn the transformation jointly with all 
      ■  As we mentioned previously, methods of structural matrix   of the model parameters.
        and transferred convolutional filters impose prior human    Proposing some general/unified approaches is one direction 
        knowledge to the model, which could significantly affect   that can be taken regarding the use of CNNs in small platforms. 
        the performance and stability. It is critical to investigate   Yuhen et al. [75] presented a feature map dimensionality reduc-
        how to control the impact of the imposed prior knowledge.  tion method by excavating and removing redundancy in feature 
      ■  The methods of KD provide many benefits such as directly   maps generated by different filters, which could also preserve 
        accelerating the model without special hardware or imple-  intrinsic information of the original network. The idea can be 
        mentations. It is still worth it to develop KD-based   extended to make CNNs more applicable for different platforms. 
        approaches and explore how to improve the performance.   The work in [76] proposed a one-shot whole network compres-
      ■  Hardware constraints in various of small platforms (e.g.,   sion scheme consisting of three components: rank selection, low-
        mobile, robotic, self-driving cars) are still a major problem   rank tensor decomposition, and fine-tuning to make deep CNNs 
        that hinder the extension of deep CNNs. How to make full   work in mobile devices. From the systematic side, Facebook 
        use of the limited computational source available and how   released the platform Caffe2 [77], which employed a particularly 
        to design special compression methods for such platforms   lightweight and modular framework and included mobile-specif-
        are still challenges that need to be addressed.          ic optimizations based on the hardware design. Caffe2 can help 
                                            developers and researchers train large machine-learning models 
      Possible solutions                           and deliver AI on mobile devices.
      To solve the hyperparameters configuration problem, we can 
      rely on the recent learning-to-learn strategy [72], [73]. This   Acknowledgments
      framework provides a mechanism, allowing the algorithm to   We would like to thank the reviewers and broader community 
      automatically learn how to exploit structure in the problem of   for their feedback on this survey. In particular, we would like 
      interest. There are two different ways to combine the learning-  to thank Hong Zhao from the Department of Automation of 
      to-learn module with the model compression. The first designs   Tsinghua University for her help on modifying this article. 
      compression and learning-to-learn simultaneously, while the   This research is supported by National Science Foundation of 
      second way first configures the model with learn-to-learning   China, grant number 61401169. The corresponding author of 
      and then prunes the parameters.                   this article is Pan Zhou. 
        Channel pruning provides the efficiency benefit on 
      both CPUs and GPUs because no special implementation is   Authors
      required. But it is also challenging to handle the input con-  Yu Cheng (chengyu@us.ibm.com) received his bachelor’s 
      figuration. One possible solution is to use the training-based   degree in automation from Tsinghua University, Beijing, 
      channel pruning methods [74], which focus on imposing sparse   China, in 2010 and his Ph.D. degree in computer science 
      constraints on weights during training, and could adaptively   from Northwestern University, Evanston, Illinois in 2015. 
      determine hyperparameters. However, training from scratch   Currently, he is a research staff member at AI Foundations Lab, 
      for such a method is costly for very deep CNNs.          IBM T.J. Watson Research Center, Yorktown Heights, New 
        Exploring new types of knowledge in the teacher models   York. His research is focused on deep learning in general, with 
      and transferring it to the student models is useful for the KD   specific interests in deep generative models and deep models 

    134                         IEEE SIgnal ProcESSIng MagazInE    |   January 2018    |       compression. He also has published many works regarding the   [12] C. Zhu, S. Han, H. Mao, and W. J. Dally, “Trained ternary quantization,” arXiv 
       applications of deep learning in computer vision and natural   Preprint, arXiv:1612.01064, 2016.
       language processing.                              [13] M. Courbariaux, Y. Bengio, and J. David, “Binaryconnect: Training deep neu-
                                                   ral networks with binary weights during propagations,” in Proc. Advances Neural 
         Duo Wang (d-wang15@mails.tsinghua.edu.cn) received the   Information Processing Systems Annu. Conf., 2015, pp. 3123–3131.
       B.S. degree in automation from the Harbin Institute of    [14] M. Courbariaux and Y. Bengio, “Binarynet: Training deep neural networks 
       Technology, China, in 2015, where he is currently pursuing his   with weights and activations constrained to +1 or −1,” Computing Res. Repository, 
                                                   vol. abs/1602.02830, 2016. [Online]. Available: https://arxiv.org/abs/1602.02830 Ph.D. degree in the Department of Automation, Tsinghua   [15] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi, “Xnor-net: Imagenet  University. His research interests are deep/machine learning and   classification using binary convolutional neural networks,” in Proc. European Conf. 
       their applications in computer vision and robotics vision.        Computer Vision, 2016, pp. 525–542. 
         Pan Zhou (panzhou@hust.edu.cn) received his B.S. degree   [16] P. Merolla, R. Appuswamy, J. V. Arthur, S. K. Esser, and D. S. Modha, “Deep 
                                                   neural networks are robust to weight binarization and other non-linear distortions,”  in the Advanced Class of Huazhong University of Science and   Computing Res. Repository, vol. abs/1606.01981, 2016. [Online]. Available: https://
       Technology (HUST), Wuhan China, and his M.S. degree in elec-  arxiv.org/abs/1606.01981 
       tronics and information engineering from the same university in   [17] L. Hou, Q. Yao, and J. T. Kwok, “Loss-aware binarization of deep networks,” 
                                                   Computing Res. Repository, vol. abs/1611.01600, 2016. [Online]. Available: https:// 2006 and 2008, respectively. He received his Ph.D. degree from   arxiv.org/abs/1611.01600 
       the School of Electrical and Computer Engineering at the   [18] Z. Lin, M. Courbariaux, R. Memisevic, and Y. Bengio, “Neural networks with 
       Georgia Institute of Technology, Atlanta in 2011. Currently, he is   few multiplications,” Computing Res. Repository, vol. abs/1510.03009, 2015. 
                                                   [Online]. Available: https://arxiv.org/abs/1510.03009 an associate professor with School of Electronic Information and   [19] S. J. Hanson and L. Y. Pratt, “Comparing biases for minimal network con- Communications, HUST. His research interests include big data   struction with back-propagation,” Adv. Neural Inform. Process. Syst. 1, 1989, pp. 
       analytics and machine learning, security and privacy, and infor-   177–185.
       mation networks.                                 [20] Y. L. Cun, J. S. Denker, and S. A. Solla, “Advances in neural information pro-
                                                   cessing systems 2,” in Optimal Brain Damage, D. S. Touretzky, Ed. San Mateo,  Tao Zhang (taozhang@mail.tsinghua.edu.cn) received his   CA: Morgan Kaufmann, 1990, pp. 598–605.
       B.S., M.S., and Ph.D. degrees from Tsinghua University,   [21] B. Hassibi, D. G. Stork, and S. C. R. Com, “Second order derivatives for 
       Beijing, China, in 1993, 1995, and 1999, respectively, and his   network pruning: Optimal brain surgeon,” in Advances in Neural Information 
                                                   Processing Systems, vol. 5. San Mateo, CA: Morgan Kaufmann, 1993, pp. 164– Ph.D. degree from Saga University, Japan, in 2002, all in con-  171. 
       trol engineering. He is a professor with the Department of   [22] S. Srinivas and R. V. Babu, “Data-free parameter pruning for deep neural net-
       Automation, Tsinghua University. His current research inter-   works,” in Proc. British Machine Vision Conf., 2015, pp. 31.1–31.12.
       ests include artificial intelligence, robotics, image processing,   [23] S. Han, J. Pool, J. Tran, and W. J. Dally, “Learning both weights and connec-
                                                   tions for efficient neural networks,” in Proc. 28th Int. Conf. Neural Information  control theory, and control of spacecraft.                 Processing Systems, 2015, pp. 1135–1143. 
                                                   [24] W. Chen, J. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen, “Compressing 
       References                                   neural networks with the hashing trick,” in Proc. Machine Learning Research 
                                                   Workshop Conf., 2015, pp. 2285–2294.[1] A. Krizhevsky, I. Sutskever, and G. Hinton, “Imagenet classification with deep 
       convolutional neural networks,” in Proc. Conf. Neural Information Processing   [25] K. Ullrich, E. Meeds, and M. Welling, “Soft weight-sharing for neural network 
       Systems, 2012, pp. 1097–1105.                             compression,” Computing Res. Repository, vol. abs/1702.04008, 2017. [Online]. 
                                                   Available: https://arxiv.org/abs/1702.04008 [2] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf, “Deepface: Closing the gap to 
       human-level performance in face verification,” in Proc. IEEE Conf. Computer   [26] V. Lebedev and V. S. Lempitsky, “Fast convnets using group-wise brain dam-
       Vision Pattern Recognition, 2014, pp. 1701–1708.                   age,” in Proc. IEEE Conf. Computer Vision Pattern Recognition, 2016, pp. 2554–
                                                   2564.[3] Y. Sun, X. Wang, and X. Tang, “Deeply learned face representations are sparse, 
       selective, and robust,” in Proc. IEEE Conf. Computer Vision Pattern Recognition,   [27] H. Zhou, J. M. Alvarez, and F. Porikli, “Less is more: Towards compact 
       2015, pp. pp. 2892–2900.                                CNNs,” in Proc. European Conf. Computer Vision, 2016, pp. 662–677.
       [4] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. Le, M. Mao, M.   [28] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li, “Learning structured sparsity in 
       Ranzato, A. Senior, P. Tucker, K. Yang, and A. Ng, “Large scale distributed deep   deep neural networks,” Adv. Neural Inform. Process. Syst., vol. 29, pp. 2074–2082, 
       networks,” in Proc. Conf. Neural Information Processing Systems, 2012, pp.   2016.  
       1223–1231.                                      [29] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf, “Pruning filters for 
       [5] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recogni-   efficient convnets,” Computing Res. Repository, vol. abs/1608.08710, 2016. 
       tion,” Computing Res. Repository, vol. abs/1512.03385, 2015. [Online]. Available:   [Online]. Available: https://arxiv.org/abs/1608.08710
       https://arxiv.org/pdf/1512.03385.pdf                          [30] Y. Cheng, F. X. Yu, R. Feris, S. Kumar, A. Choudhary, and S.-F. Chang, “An 
       [6] Y. Gong, L. Liu, M. Yang, and L. D. Bourdev, “Compressing deep convolutional   exploration of parameter redundancy in deep networks with circulant projections,” in 
       networks using vector quantization,” Computing Res. Repository, vol.   Proc. Int. Conf. Computer Vision, 2015, pp. 2857–2865.
       abs/1412.6115, 2014. [Online]. Available: https://arxiv.org/pdf/1412.6115.pdf      [31] Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. Smola, L. Song, and Z. 
       [7] Y. W. Q. H. Jiaxiang Wu, C. Leng, and J. Cheng, “Quantized convolutional neu-   Wang, “Deep fried convnets,” in Proc. Int. Conf. Computer Vision, 2015, pp. 1476–
       ral networks for mobile devices,” in Proc. IEEE Conf. Computer Vision Pattern   1483.
       Recognition, 2016, pp. 4820–4828.                           [32] V. Sindhwani, T. Sainath, and S. Kumar. (2015). Structured transforms for 
       [8] V. Vanhoucke, A. Senior, and M. Z. Mao, “Improving the speed of neural net-   small-footprint deep learning. Advances in Neural Information Processing 
       works on cpus,” in Proc. Conf. Neural Information Processing Systems Deep   Systems, 28, pp. 3088–3096. [Online]. Available: http://papers.nips.cc/paper/5869-
       Learning and Unsupervised Feature Learning Workshop, 2011.             structured-transforms-for-small-footprint-deep-learning.pdf
       [9] S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan, “Deep learning   [33] J. Chun and T. Kailath, Generalized Displacement Structure for Block-
       with limited numerical precision,” in Proc. 32nd Int. Conf. Machine Learning,   Toeplitz, Toeplitz-Block, and Toeplitz-Derived Matrices. Berlin, Germany: 
       2015, vol. 37, pp. 1737–1746.                             Springer, 1991, pp. 215–236.
       [10] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing deep neural   [34] M. V. Rakhuba and I. V. Oseledets. (2015). Fast multidimensional convolution 
       networks with pruning, trained quantization and Huffman coding,” in Proc. Int.   in low-rank tensor formats via cross approximation. SIAM J. Sci. Comput., 37(2). 
       Conf. Learning Representations, 2016.                         [Online]. Available: http://dx.doi.org/10.1137/140958529
       [11] Y. Choi, M. El-Khamy, and J. Lee, “Towards the limit of network quantiza-   [35] R. Rigamonti, A. Sironi, V. Lepetit, and P. Fua, “Learning separable filters,” 
       tion,” Computing Res. Repository, vol. abs/1612.01543, 2016. [Online]. Available:   in Proc. IEEE Conf. Computer Vision Pattern Recognition, 2013, pp. 2754–
       https://arxiv.org/abs/1612.01543                            2761.


                                     IEEE SIgnal ProcESSIng MagazInE    |   January 2018    |                             135                                                                      [36] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus, “Exploiting lin-                             [57]  A.  Almahairi,  N.  Ballas,  T. Cooijmans,  Y. Zheng,  H.  Larochelle, and A. C. 






                                                                      ear structure within convolutional networks for efficient evaluation,”  Adv. Neural                          Courville, “Dynamic capacity networks,” in Proc. 33rd Int. Conf. Machine Learning, 






                                                                      Inform. Process. Syst. vol. 27, pp. 1269–1277, 2014.                                                                                                                                                                              2016, pp. 2549–2558.











                                                                      [37] M. Jaderberg, A. Vedaldi, and A. Zisserman, “Speeding up convolutional neu-                             [58] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. 






                                                                      ral networks with low rank expansions,” in Proc. British Machine Vision Conf.,                            (2017).  Outrageously large neural networks: The sparsely-gated mixture-of-experts 






                                                                      2014, pp. 1–13.                                                                                                                                                                                                                                                                                                                                                         layer. [Online]. Available: https://openreview.net/pdf?id=B1ckMDqlg











                                                                      [38]  V.  Lebedev,  Y.  Ganin,  M.  Rakhuba,  I. V.  Oseledets, and V. S.  Lempitsky,                            [59]  D.  Wu,  L.  Pigou,  P.  Kindermans,  N. D.  Le,  L.  Shao,  J.  Dambre, and J. 






                                                                      “Speeding-up convolutional neural networks using fine-tuned CP-decomposition,”                           Odobez, “Deep dynamic neural networks for multimodal gesture segmentation and 






                                                                      Computing Res. Repository, vol. abs/1412.6553, 2014. [Online]. Available: https://                             recognition,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 38, no. 8, pp. 1583–






                                                                      arxiv.org/abs/1412.6553                                                                                                                                                                                                                                                                                                                1597, 2016. 











                                                                      [39]  C.  Tai,  T.  Xiao,  X.  Wang, and E. Weinan, “Convolutional neural networks                          [60]  C.  Szegedy,  W.  Liu,  Y.  Jia,  P.  Sermanet,  S.  Reed,  D.  Anguelov,  D.  Erhan,  V. 






                                                                      with low-rank regularization,” Computing Res. Repository, vol. abs/1511.06067,                            Vanhoucke, and A. Rabinovich. (2015). Going deeper with convolutions. Proc. IEEE 






                                                                      2015.                                                                                                                                                                                                                                                                                                                                                                                                       Computer Vision Pattern Recognition. [Online]. Available: http://arxiv.org/







                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       abs/1409.4842



                                                                      [40]  M.  Denil,  B.  Shakibi,  L.  Dinh,  M.  Ranzato, and N. D.  Freitas. (2013). 







                                                                      Predicting parameters in deep learning.    Advances in Neural Information                           [61] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, “Deep networks 






                                                                      Processing Systems, 26, 2148 –2156. [Online]. Available: http://media.nips.cc/nips-                             with stochastic depth,” Computing Res. Repository, vol. arXiv:1603.09382, 






                                                                      books/nipspapers/paper_files/nips26/1053.pdf                                                                                                                                                                                                         2016. 











                                                                      [41] T. N. Sainath, B. Kingsbury, V. Sindhwani, E. Arisoy, and B. Ramabhadran,                            [62] Y. Yamada, M. Iwamura, and K. Kise. (2016). Deep pyramidal residual networks 






                                                                      “Low-rank matrix factorization for deep neural network training with high-dimen-                            with separated stochastic depth, Computing Res. Repository, vol. abs/1612.01230. 






                                                                      sional output targets,” in  Proc. IEEE Int. Conf. Acoustics Speech Signal                             [Online]. Available: http://arxiv.org/abs/1612.01230






                                                                      Processing, 2013, pp. 6655–6659.



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [63] M. Mathieu, M. Henaff, and Y. Lecun, “Fast training of convolutional networks 






                                                                      [42]  T. S.  Cohen and M.  Welling, “Group equivariant convolutional networks,”                           through FFTs,” Computing Res. Repository, vol. arXiv:1312.5851, 2014. 






                                                                      arXiv Preprint, arXiv:1602.07576, 2016.



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [64] A. Lavin and S. Gray, “Fast algorithms for convolutional neural networks,” 






                                                                      [43] S. Zhai, Y. Cheng, and Z. M. Zhang, “Doubly convolutional neural networks,” in                         in  Proc. IEEE Conf. Computer Vision Pattern Recognition ,  2016, pp. 4013 –






                                                                     Proc. Advances Neural Information Processing Systems, 2016, pp. 1082–1090.                                                            4021.











                                                                     [44] W. Shang, K. Sohn, D. Almeida, and H. Lee, “Understanding and improving con-                             [65] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied 






                                                                     volutional neural networks via concatenated rectified linear units,” arXiv Preprint,                            to document recognition,” Proc. IEEE, pp. 2278–2324, 1998. 






                                                                     arXiv:1603.05201, 2016.



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [66] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. A. Riedmiller, “Striving for 







                                                                     [45] H. Li, W. Ouyang, and X. Wang, “Multi-bias non-linear activation in deep neural                         simplicity: The all convolutional net,” Computing Res. Repository, vol. abs/1412.6806, 







                                                                     networks,” arXiv Preprint, arXiv:1604.00676, 2016.                                                                                                                                                                                         2014. [Online]. Available: https://arxiv.org/abs/1412.6806











                                                                     [46]  S. Dieleman, J. D Fauw, and K. Kavukcuoglu, “Exploiting cyclic symmetry in                         [67]  M.  Lin,  Q.  Chen, and S.  Yan , “Network in network,” in Proc. Int. Conf. 







                                                                     convolutional neural networks,” in Proc. 33rd Int. Conf. Machine Learning, 2016, vol.                           Learning Representations,   2014. [Online]. Available: https://arxiv.org/abs/ 







                                                                     48, pp. 1889–1898.                                                                                                                                                                                                                                                                                                                                           1312.4400 











                                                                     [47] C. Szegedy, S. Ioffe, and V. Vanhoucke. (2016). Inception-v4, inception-resnet and                           [68]  K.  Simonyan and A.  Zisserman, “Very deep convolutional networks for large-







                                                                     the impact of residual connections on learning,  Computing Res. Repository, vol.                          scale image recognition,” Computing Res. Repository, vol. abs/1409.1556,  2014. 







                                                                     abs/1602.07261. [Online]. Available: http://dblp.uni-trier.de/db/journals/corr/corr1602.                             [Online]. Available: https://arxiv.org/abs/1409.1556







                                                                     html#SzegedyIV16



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [69] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recogni-







                                                                     [48] B. Wu, F. N. Iandola, P. H. Jin, and K. Keutzer, “Squeezedet: Unified, small, low                          tion,” arXiv Preprint, arXiv:1512.03385, 2015.







                                                                     power fully convolutional neural networks for real-time object detection for autono-



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [70] Y. Cheng, F. X. Yu, R. S. Feris, S. Kumar, A. N. Choudhary, and S. Chang, “An 


                                                                     mous driving,” Computing Res. Repository, vol. abs/1612.01051,  2016. [Online]. 



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       exploration of parameter redundancy in deep networks with circulant projections,” in 


                                                                     Available: https://arxiv.org/abs/1612.01051



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Proc. IEEE Int. Conf. Computer Vision, 2015, pp. 2857–2865.







                                                                     [49]  C.  Buciluaˇ,  R.  Caruana, and A. Niculescu-Mizil. (2006).  Model compression. 



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [71] M. Moczulski, M. Denil, J. Appleyard, and N. de Freitas, “ACDC: A structured 


                                                                     Proc. 12th ACM SIGKDD Int. Conf. Knowledge Discovery Data Mining, pp. 535–



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       efficient linear layer,” in Proc. Int. Conf. Learning Representations, 2016.


                                                                     541. [Online]. Available: http://doi.acm.org/10.1145/1150402.1150464







                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [72] M. Andrychowicz, M. Denil, S. G. Colmenarejo, M. W. Hoffman, D. Pfau, T. 


                                                                     [50] J. Ba and R. Caruana, “Do deep nets really need to be deep?” Adv. Neural Inform. 



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Schaul, and N.  de Freitas, “Learning to learn by gradient descent by gradient 


                                                                     Process. Syst., vol. 27, pp. 2654–2662, 2014. 



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       descent,” in Proc. Neural Information Processing Systems Conf., 2016, pp. 3981–







                                                                     [51] G. E. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural net-                            3989. 







                                                                      work,” Computing Res. Repository, vol. abs/1503.02531, 2015. [Online]. Available: 



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [73] D. Ha, A. Dai,  and Q. Le, “Hypernetworks,” in Proc. Int. Conf. Learning 


                                                                     https://arxiv.org/abs/1503.02531



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Representations, 2016. 






                                                                     [52]  A.  Romero,  N.  Ballas,  S. E.  Kahou,  A.  Chassang,  C.  Gatta, and Y.  Bengio, 



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [74] J. M. Alvarez and M. Salzmann, “Learning the number of neurons in deep net-


                                                                      “Fitnets: Hints for thin deep nets,” Computing Res. Repository, vol. abs/1412.6550, 



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       works,” in Proc.  Neural Information Processing Systems Conf.,  2016, pp. 2270–


                                                                      2014. [Online]. Available: https://arxiv.org/abs/1412.6550



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       2278. 






                                                                      [53] A. Korattikara Balan, V. Rathod, K. P. Murphy, and M. Welling. (2015). Bayesian 



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [75] Y. Wang, C. Xu, C. Xu, and D. Tao, “Beyond filters: Compact feature map for 


                                                                      dark knowledge. Advances in Neural Information Processing Systems, 28, 3420–3428. 



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       portable deep model,” in Proc. 34th Int. Conf. Machine Learning, 2017, pp. 3703–


                                                                      [Online]. Available: http://papers.nips.cc/paper/5965-bayesian-dark-knowledge.pdf



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       3711.






                                                                      [54] P. Luo, Z. Zhu, Z. Liu, X. Wang, and X. Tang, “Face model compression by dis-



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [76] Y.-D. Kim, E. Park, S. Yoo, T. Choi, L. Yang, and D. Shin, “Compression of deep 


                                                                      tilling knowledge from neurons,” in Proc. 30th AAAI Conf. Artificial Intelligence, 



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       convolutional neural networks for fast and low power mobile applications,” Computing 


                                                                      2016, pp. 3560–3566.



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Res. Repository, vol. abs/1511.06530,  2015. [Online]. Available: https://arxiv.org/






                                                                      [55]  T.  Chen,  I. J.  Goodfellow, and J.  Shlens, “Net2net: Accelerating learning via                          abs/1511.06530






                                                                      knowledge transfer,” Computing Res. Repository, vol. abs/1511.05641, 2015. [Online]. 



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [77] Facebook, Inc. Caffe2: A new lightweight, modular, and scalable deep learning 


                                                                      Available: https://arxiv.org/abs/1511.05641



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       framework. (2016). [Online]. Available: https://caffe2.ai/ 






                                                                      [56]  S.  Zagoruyko and N.  Komodakis. (2016).  Paying more attention to attention: 







                                                                      Improving the performance of convolutional neural networks via attention transfer, 







                                                                      Computing Res. Repository, vol. abs/1612.03928. [Online]. Available: http://arxiv.org/







                                                                      abs/1612.03928                                                                                                                                                                                                                                                                                                                                                                                                  SP









     136                             IEEE SIgnal ProcESSIng MagazInE    |   January 2018    |